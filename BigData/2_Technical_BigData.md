2_Technical_BigData

<p align="center">
<img src="https://user-images.githubusercontent.com/40620421/183598835-ebf8d4ba-354d-4b02-8ccd-a7d1f2d29018.jpg" width="300">

본 내용은 해당 도서를 참고하여 정리한 내용입니다.
</p>

<br/>
<br/>
<br/>

# 2. 빅데이터의 탐색

## 2-1. 크로스 집계의 기본
- 데이터 시각화에서 먼저 기본이 되는 것이 크로스 집계
- `크로스 집계`: 트랜잭션 테이블에서 크로스 테이블로 변환하는 과정
    1. 크로스 테이블: 행과 열이 교차하는 부분에 숫자 데이터가 들어가는 테이블
    1. 트랜잭션 테이블: 행을 기준으로 데이터가 추가되는 테이블

<img src="https://user-images.githubusercontent.com/40620421/185924635-c1f3f12d-b655-49ee-9382-6ba4b359acf0.png" width="500">

### SQL에 의한 테이블 집계
- 대량의 데이터를 크로스 집계하기 위해 SQL을 활용
    - 데이터 양이 너무 많으면 피벗 테이블을 통한 크로스 집계가 어려움
- SQL을 통해서 크로스 테이블이 아닌 트랜잭션 테이블의 형태로 얻고, 이를 크로스 집계함으로써 임의의 크로스 테이블을 얻을 수 있음
```sql
SELECT 매출일, 점포 ID, 금액 FROM 판매이력
```
매출일|점포ID|고객ID|금액
--|--|--|--
2022-08-01|1|10|60000
2022-08-01|1|11|50000
2022-08-01|2|12|30000
2022-08-02|1|13|120000
2022-08-02|2|14|50000
2022-08-02|2|15|40000

점포ID\매출일|2022-08-01|2022-08-02
--|--|--
1|110000|120000
2|30000|90000

- 데이터 집계를 `SQL`로 하고, 크로스 집계를 `시각화 도구`로 함으로써 이론상 무한히 많은 데이터가 있더라도 크로스 집계가 가능하다
    - `데이터 집계의 프로세스`: SQL로 집계
    - `시각화 프로세스`: 시각화 도구로 크로스 집계

## 2-2. 열 지향 스토리지에 의한 고속화
대량의 데이터를 신속하게 집계하기 위한 DB 구조



### DB 지연 줄이기
- RDB는 메모리가 부족하면 급격히 성능이 저하된다
- `MPP 기술`
    - 고속화를 위해 사용되는 기법이 `압축과 분산`
    - 데이터를 최대한 압축한 후, 여러 디스크에 분산함으로써 데이터 로드의 지연을 줄인다
    - 멀티 코어를 활용하여 디스크 I/O를 병렬처리
    - 하드웨어 수준에서 균형된 CPU와 디스크를 가진 DB를 `MPP 데이터베이스`라고 함
    - Amazon RedShift, Google BigQuery

### 열 지향 DB 접근

#### 행 지향 DB
- 일반적으로 많이 사용되는 DB
- 데이터 검색을 위해서는 모든 레코드를 가져와야함 → 성능 저하
- `Index`를 통해 데이터 검색을 고속화
- 하지만 데이터 분석에서는 어떤 칼럼이 사용되는지 미리 알 수 없기 때문에 Index가 큰 도움이 되지 않음

#### Index
- 장점
    - 테이블을 조회하는 속도와 성능 향상
    - 시스템 부하를 줄임
- 단점
    - 인덱스를 관리하기 위한 추가 저장 공간 필요
    - 인덱스를 항상 정렬된 상태로 유지하기에 추가 작업이 필요
    - 인덱스를 잘못 사용할 경우 역효과가 날 수 있다.
- Index 권장 케이스
    - 데이터 양이 많은 테이블
    - 업데이트보다 조회가 잦은 테이블
    - 조건문이나 정렬이 잦은 테이블
- Index 자료 구조
    - Hash 인덱스 알고리즘, B-Tree 인덱스 알고리즘, B+Tree 인덱스 알고리즘 (균형 트리)

#### 열 지향 DB
- 데이터를 미리 `칼럼 단위`로 정리해 둠으로써 필요한 칼럼만 로드하여 디스크 I/O를 줄임
- 일부 칼럼만이 집계 대상이 될 때 유용함
- `압축 효율`이 우수함 (같은 칼럼에는 유사한 데이터가 나열되기 때문)
- `MPP`를 통해 하나의 쿼리를 다수의 작은 태스크로 분해하고 병렬로 실행
    - 디스크로부터 로드가 병목 현상이 발생하지 않도록 데이터가 고르게 분산되어 있어야함

## 2-3. 애드 혹 분석과 시각화 도구

### 대시보드 도구
- 새로운 그래프를 쉽게 추가할 수 있는 도구
- 최신의 집계 결과를 즉시 확인할 수 있길 기대
- 정해진 지표의 일상적인 변화를 모니터링

### BI 도구
- 대화형 데이터 탐색이 중요시됨
- ex. 몇개월 단위의 장기적인 데이터 추이를 시각화, 집계의 조건을 세부적으로 바꿈
- Tableau

## 2-4. 데이터 마트의 기본 구조
BI 도구에서 대화형으로 데이터를 참고하려면, 시각화 정보만을 모은 데이터 마트가 필수

### 시각화에 적합한 데이터 마트 만들기
- `OLAP`/online analytical processing
    - 사용자가 다차원 정보에 접근하여 대화식으로 정보를 분석하도록 지원
- `다차원 모델`
    - 데이터 집계를 효율화하는 접근 방법 중 하나
    - 데이터 분석을 위해 만들어진 다차원 데이터를 `OLAP 큐브`라고 하고, 그것을 크로스 집계하는 구조가 `OLAP`
    - 크로스 집계의 모든 조합을 미리 계산하여 DB안에 캐시해두고, 쿼리가 실행되면 집계된 결과를 반환하는 구조
    - 데이터 마트가 이전에 가지고 있던 구조
- `MPP DB와 비정규화 테이블`
    - MPP DB와 인 메모리 DB 등의 보급으로 사전에 계산할 필요가 없음
    - MPP DB에 다차원 모델의 개념이 없기 때문에 비정규화 테이블을 준비
    - 시각화에 적합한 데이터 마트를 만드는 것은 BI 도구를 윈한 `비정규화 테이블`을 만드는 프로세스

### 테이블을 비정규화하기

#### 정규화 테이블
- `트랜잭션 테이블`: 시간과 함께 생성되는 데이터
    - 시간과 함께 생성되기에 변하지 않음
- `마스터 테이블`: 트랜잭션에서 참고되는 각종 정보
    - 상황에 따라 일부 데이터가 업데이트됨
    - 고객 테이블에서 고객ID는 변경되지 않지만, 고객 주소와 같은 속성은 변경될 수 있음

#### 비정규화 테이블
- 과거
    - DW에서는 트랜잭션과 마스터를 `팩트 테이블`과 `디멘젼 테이블`이라고 칭함
    - DM를 만들 때 팩트 테이블을 중심으로 여러 디멘젼 테이블을 결합하는 것이 좋음 (`스타 스키마`)
- 현재
    - MPP DB와 열 지향 스토리지 시스템의 발달로 칼럼수가 아무리 늘어나도 성능에 영향을 주지 않는다
    - 처음부터 팩트 테이블에 모든 칼럼을 포함해두고, 쿼리 실행 시 테이블 결합을 하지 않는다
    - 장점
        1. 빠른 데이터 조회 (Join 비용이 줄어들기 때문)
        1. 데이터 조회 쿼리의 간단화
    - 단점
        1. 데이터 갱신이나 삽입 비용이 높음
        1. 데이터 무결성을 해침
        1. 데이터 중복 저장으로 인한 추가 저장공간 확보

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbJUNWP%2FbtqE2twgz5B%2FZ9qC1T74uCQ1CyAbKnUG60%2Fimg.png" width="500">

<br/>
<br/>
<br/>


# 3. 빅데이터의 분산 처리

## 3-1. 대규모 분산 처리 프레임워크
다수의 컴퓨터에 데이터 처리를 분산하기 위해서는 그 실행을 관리하는 프레임 워크가 필요하다.  
`Hadoop`, `Spark` 중심

### 데이터 구조화

<img src="https://user-images.githubusercontent.com/40620421/186179295-30d91728-6b4a-4246-bc20-d3c544b77397.png" width="500">

- `구조화 데이터`: 스키마가 명확하게 정의된 데이터
    - 스키마: 테이블의 칼럼 명, 데이터 타입, 테이블 간의 관계 등을 정의
    - 기존 DW에서 데이터를 축척하는 일반적인 방식
- `비구조화 데이터`: 스키마가 없는 데이터
    - 자연 언어로 작성된 텍스트 데이터, 이미지, 동영상 등
    - 이 상태로는 SQL로 제대로 집계할 수 없음
    - 이 데이터를 분산 스토리지 등에 저장하고 그것을 분산 시스템에서 처리하는 것이 `데이터 레이크`의 개념
- `스키마리스 데이터`: 기본 서식은 있지만, 스키마가 정의되지 않음
    - CSV, JSON, XML 등
    - 칼럼 수나 데이터 타입은 명확하지 않음
    - NoSQL DB에서 활용

#### 열 지향 스토리지
- `MPP DB`: 제품에 따라 스토리지 형식이 고정되어 사용자가 상세히 몰라도됨
- `Hadoop`: 직접 열 지향 스토리지 형식을 선택
    - `ORC`: 구조화 데이터를 위한 열 지향 스토리지. 처음 스키마를 정한 후 데이터 저장.
    - `Parquet`: 스키마리스에 가까운 데이터 구조. JSON과 같은 데이터도 그대로 저장 가능.

### Hadoop
- 대규모 분산시스템을 구축하기 위한 공통 플랫폼
- 분산 시스템을 구성하는 다수의 소프트웨어로 이루어진 집합체 (단일 소프트 웨어 X)
- Why Hadoop? 🤔
    1. 접근성과 비용 절감  
    접근성이 뛰어나서 유연한 방식으로 하드웨어를 사용할 수 있음 → 고가의 신뢰도 높은 하드웨어 만을 추구하지 않음 → 비용 절감
    기존 DB는 소프트웨어와 하드웨어가 비싸다
        
    2. 빅데이터에 대한 내결함성  
    기본적으로 HDFS는 파일을 `3군데 저장 `
    → 공간적으로 비효율적이다? 만약 한 데이터 노드에서 데이터를 구성할 수 있다면? 성능적 이득을 볼 수 있음
    → 왜 3개? 내결함성+성능 실험적으로 알맞음
        
    3. 확장성  
    하드웨어를 추가했을 때, 성능이 리니어하게 증가한다.
        
    4. 읽기 시점 스키마  
    기존에 쓰기 시점 스키마에서 벗어나, 하둡이나 NoSQL은 데이터를 읽을 때 데이터의 본질을 파악한다.
    데이터를 소비자에게 데이터의 성질을 맡기는 시스템. (데이터로 뭘하기 전까진 데이터로 무엇을 할지 정확히 알 수 없다.)
- Why Java? 🤔
    1. Java의 GC가 가장 성능이 좋음 (빅데이터를 다루기에 메모리 관점 중요)
    2. Java가 디버깅이 쉬움
    3. Hadoop이 Nutch 프로그램에서 발전 되었는데, 그것이 Java 기반
- 하둡 2 vs 3
    - 이레이저코딩 도입하여 기존의 블록 복제를 대체하는 방식   
        ⇒ 1GB 저장: 3GB → 1.5GB  
        ⇒ 데이터 손실 시 복구할 수 있는 기법. 데이터 복제를 대체하지는 않음
    - 2개 이상의 네임노드를 지원

참고  
https://intrepidgeeks.com/tutorial/hedou-you-should-know  
https://deep-flame.tistory.com/9

#### HDFS (Hadoop Distributed File System)
- 데이터를 블록으로 나누어 저장하는 `분산 파일 시스템`
- 수정, 삭제 불가 → 한 번쓰고, 여러 번 읽는 구조에 적합함
- 마스터-워커 패턴 (Name Node가 쇼를 하고, Data Node가 모든 일을 한다)
    - `Name Node`
        메타 데이터 (모든 파일과 디렉토리에 대한 정보)
        Name Node가 없다면 Data Node에 있는 데이터를 불러올 수 없음
        따라서 Name Node의 정보를 지속적으로 백업

    - `Data Node`
        클라이언트나 네임노드의 요청에 블록을 저장하고, 탐색
- `HDFS의 기본 블록 사이즈 128MB` 왜 이렇게 큼?
    - 탐색 비용 최소화: 블록의 시작점을 탐색하는 데에 걸리는 시간을 줄일 수 있음

    - 전송 비용 최소화: 한 데이터 노드에서 데이터를 모두 구성할 수 있다면 데이터 블락을 한데로 모으는 시간을 최소화할 수 있음

    - 대용량 데이터를 기반으로 함

<img src="https://blog.kakaocdn.net/dn/baTHVO/btrqsdMTHNX/bChgeqd831xlqEc1naDzR0/img.gif" width="500">

```
Write-1. 데이터를 블록으로 나눈다.
Write-2. 블록을 저장하려는 클라이언트는 네임노드를 통해 블록이 저장될 데이터 노드의 목록을 받는다.
Write-3. 클라이언트는 첫 번째 데이터노드에 블록을 쓰고, 차례대로 다음 데이터노드로 파이프라인을 통해 데이터를 흘려보낸다. (네임노드는 데이터노드의 가용 저장 공간을 고려하여 파이프라인을 구성)
Read-1. 네임노드를 통해 읽을 블록을 가지고 있는 데이터 노드의 목록을 받는다.
Read-2. 최대한 가까운 곳에 위치한 데이터노드로부터 데이터를 읽어들인다.
```

#### YARN (Yet Another Resource Negotiator)
- CPU나 메모리 등의 계산 리소스를 관리하는 시스템
- 리소스 매니저와 노드 메니지가 서로 통신하며 통해 수많은 워크 노드의 리소스를 관리
    - `리소스 매니저`  
        실행되는 애플리케이션을 예약하는 중앙 관리 시스템
    - `노드 매니저`  
        컨테이너를 시작하고 리소스 사용량(CPU, 메모리, 디스크, 네트워크)를 모니터링하고 리소스 매니저에 보고

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBN83W%2FbtrqnklKtxC%2FRLCe086rprV5phLHIIruFK%2Fimg.png" width="500">

```
Assign-1. 클라이언트가 리소스 매니저에 어플리케이션을 제출한다.
Assign-2. 워커 노드 내 어플리케이션 마스터는 리소스 매니저에게 어플리케이션을 실행할 리소스 할당을 요청한다. 
Assign-3. 이때 연산 수행에 필요한 자원은 서로 다른 컨테이너 단위로 분할되어 어플리케이션 마스터에게 전달된다.

Run-1. 어플리케이션 마스터는 노드 메니저에게 컨테이너의 실행 명령을 전달한다.
Run-2. 코드가 컨테이너에서 실행된다.
Run-3. 어플리케이션이 종료되면 어플리케이션 마스터는 리소스메니저에서 자신을 제거하고 셧다운된다.
```

#### ZooKeeper
- Hadoop의 분산처리 환경에서 `조율을 관리하는 코디네이션 서비스`
- 복수의 컴퓨터가 네트워크를 통해 통신하며 하나의 목적을 위해 서로간에 상호작용한다. 이때 마치 하나인 것처럼 동작하는 시스템처럼 합의를 이끌어내는 서비스
- 고가용성 확보, 태스크 조율, 상태 추적, 일반적인 설정 파라미터 값 지정 수행

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkPeUK%2FbtrKvEx2xrY%2FPfX6CkmkvXsuIuRhETmKNk%2Fimg.png" >

아키
1. 클라이언트가 주키퍼 서버에 데이터를 업데이트한다. (Client B > Server D)
2. Leader 서버에서 이를 알린다.
3. Leader 서버에서는 Broadcast 형식으로 Follower 서버들에게 알린다.
4. 모든 서버에서 데이터가 일관된 상태로 유지된다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F1jbGE%2FbtrKsO2SjhG%2FLknJjwgxd0kAVJRWwCRLZk%2Fimg.jpg">

`znode`
- 데이터를 저장하기 위해 사용하는 가장 작은 단위의 데이터 저장 객체
- 모든 데이터가 메모리에 저장되며, 최대 1MB로 제한적 → `설정 값이나 리소스 상태` 등을 저장하는 매우 유용
- Zookeeper는 여러 서버에 분산되어 있는 znode를 관리한다.
- 종류
    - Persistent Node: 명시적으로 삭제되기 전까지 존재.
    - Ephemeral Node: 세션이 유지되는 동안 존재. 자식 노드를 가질 수 없다


#### MapReduce
- 분산 시스템에서 데이터 처리를 하는 시스템
- 비구조화 데이터를 가공하는 데 적합
- `Map Task`: 파일에 있는 각 레코드를 Key-Value형태로 변환한 결과를 반환한다.
- `Reduce Task`: 하나의 키에 대한 여러 값의 집계 또는 결합해서 입력값의 개수보다 더 작은 개수의 결과값을 산출한다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhfcnK%2FbtrqsFwXfmJ%2F46fgAZoKuSSWyViDgDMVMK%2Fimg.png" width="500">

```
예) 단어 개수 세기
Map. (단어, 1)의 Key-Value 구조의 리스트를 반환한다.
Shuffling. 단어 중심으로 데이터를 데이터를 모은다.
Reduce. (단어, count)를 수행하여 각 블록에서 특정 단어가 몇 번 나왔는지 계산한다.
```

#### Hive
- SQL 등의 쿼리 언어에 의한 데이터 집계 가능 (SQL-on-Hadoop)
    - 데이터가 저장된 HDFS에 접근하기 어려움
- 쿼리를 자동으로 MapReduce 프로그램으로 변환하는 소프트웨어로 개발됨 (Spark도 지원되긴 함)  
    → MapReduce가 대량의 배치 처리를 위한 시스템이다.  
    → `초기 지연이 너무 크기` 때문에 작은 쿼리 실행에는 적합하지 않다.  
    → `배치 처리`에 적합  
- Hive는 DB가 아닌 데이터 처리를 위한 배치 처리 구조이다. (쿼리 성능 고민 필요)

👉 `메타 스토어`
- HDFS에 적재된 데이터의 메타정보(파일 위치, 이름 등)을 Table Schema 정보와 함께 메타스토어에 등록  
→ Hive 쿼리를 수행할 때 메타스토어의 정보를 참조하여 마치 RDBMS에서 데이터를 조회하는 것 같은 기능 제공

👉 `아키텍처`  

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMBATj%2FbtrKwbh45zy%2FD0q0JwtPs5MKOUFCPbKG41%2Fimg.png" width="500">

하이브는 사용자 쿼리를 파싱하고, `최적화`해서 하나 이상의 연쇄 배치 연산으로 `컴파일`하며 이를 클러스터에서 실행한다. 
1. HiveQL 문을 Driver가 받고, 메타스토어의 정보를 활용하여 적합한 형태로 컴파일
2. 컴파일된 SQL을 실행 엔진으로 실행
3. 리소스 매니저가 클러스터 자원을 적절히 활용하여 실행
4. 원천 데이터는 HDFS를 활용
5. 결과를 사용자에게 변환 

👉 `데이터 저장 방법`  
- **SerDe**: Hive가 데이터를 해석하는 방법을 제공
    - Avro, ORC, RegEx, Thrift, Parquet, CSV, JSONSerDe
    - Parquet: 열 기반 데이터 구조 (인코딩 효율이 높음, 쿼리 성능이 높음)
- 쿼리 성능 향상을 위한 데이터 형식
    - **Partition**
        - Column 정보를 이용하여 폴더 단위로 데이터가 생성 (큰 데이터를 작은 데이터로 쪼갬)
        - 기본적으로 테이블의 모든 row를 읽음 (파티션이 있다면 폴더의 데이터만 읽어 성능 향상)
        - Partition 범주가 너무 많지 않도록 주의
    - **Bucket**
        - 지정된 칼럼의 값을 해쉬 처리하고, 데이터를 지정한 수의 파일로 나누어 저장
        - 범주가 40이고, 버켓이 20이면 한 버켓이 2개의 범주씩 쌓인다
        - 조인 키로 버킷을 생성해두면 생성된 버킷 중 필요한 버킷만 조회하면 됨 → 성능 향상
    - **Skew**
        - 주로 많이 들어오는 데이터가 몰릴 때 사용 (A, B 외 나머지 총 3개의 디렉토리나 파일로 구별)
        - 네임노드의 관리 포인트가 줄어든다.


#### Impala, Presto
- 대화형의 쿼리 실행 전문
- 초기 지연이 적음 → Hive를 통해서 구조화 데이터를 만들고, 뒤에 활용됨

👉 **Impala**
1. SQL, HiveQL 모두 사용 가능
2. HDFS에 저장 + Hive 메타스토어를 사용
3. C++을 통해 구현하여 속도 개선 + 실시간 데이터 처리 + `데이터 소비자` 
4. 새로 프로젝트를 시작한다면 좋은 옵션

👉 **Presto**
1. 페타바이트 급의 데이터처리 SQL 사용
2. 대기 시간에 최적화, 쿼리 처리량 메모리 양에 제한 + `데이터 소비자` 
3. 기존의 진행 중인 데이터 시스템을 수정할 필요없이 기존의 생태계와 원활하게 통합되도록 설계
4. 컴퓨팅과 스토리지가 별도로 수행되어서 클라우드 환경에 적합하다.


### Spark
- MapRduce의 단점을 극복하기 위한 데이터 처리 시스템
    - Map Reduce의 단점
        - 맵리듀스로 복잡한 파이프라인을 조합하는 것은 많은 분석가들에게 부담
        - 많은 양의 디스크 기반 I/O를 수행한다. 따라서 다중 단계 파이프라인은 I/O 비용이 매우 많이 든다.
- 대량의 `메모리를 활용`하여 고속화를 실행
    - 가능한 많은 데이터를 메모리상에 올린 상태로 두어 디스크에는 아무것도 기록하지 않는다.
    - 컴퓨터가 비정상 종료하는 경우에도, 그때 처리를 다시 시도해서 중간 데이터를 다시 생성하면 된다.
- 특징
    - JDK 필요. Spark SQL 이용
    - 다양한 스크립트 언어 지원(Java, Scala, Python, R 등)
    - MapReduce 개선 사항으로 설계됨 → Hadoop과 호환성이 좋음

👉 **RDD vs DataFrame vs Dataset**
- `RDD (Resilient Distributed Datasets)`
    - 스키마가 없는 분산된 데이터 모음 (스키마 수동 정의)
    - 메모리 내부에서 데이터 손실하더라도 재연산으로 복구할 수 있는 데이터 집합 → 내결함성 방식
    - Lineage(DAG) + 읽기전용 → 데이터 처리의 관계성을 명확히 할 수 있음 → 특정 RDD에서 메모리가 유실되더라도 복기할 수 있음
- `DataFrame`
    - 분산된 데이터를 Column으로 모은 데이터 구조
    - 관계형 테이블과 같은 구조 → 대용량 데이터를 좀 더 쉽게 처리
    - 스키마 자동 정의
- `DataSet`
    - DataFrame에서의 확장 (Dataset[Row] == DataFrame)
    - Scala and Java에서만 활용가능
    - 스키마 자동 정의

👉 **Spark 연산 과정**  
- Logical Plan
    연산 단계에서 사용될 DataFrame이나 Column이 실제로 존재하는지에 대한 검사를 시행한다.
- Physical Plan
    1. 클러스터에서 Logical Plan을 어떻게 실행할지 정의한다. 
    2. 그 방법은 다양한 방법을 시도해보고, Cost Model을 이용하여 비교한다. 
    ⇒ `Best Physical Plan`을 선택

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbOYdlK%2Fbtrq7ao3mSU%2FLbu13eBKp3vQuPfkamwjU0%2Fimg.jpg">

👉 **Lazy Ealuation**  
`Action이 시작되는 시점`에 Transformation끼리 연계를 파악해 실행 계획을 최적화 (physical plan)
- Transformation: 새로운 RDD를 생성하는 동작 (map, filter, distinct)
- Action: 기록된 모든 작업을 실제로 수행하는 연산 (first, show, collect, count)

<img src="https://user-images.githubusercontent.com/40620421/186442906-7503b6ce-e867-43c2-9fa8-17fba7b7d9c7.png" width="500">

```scala
// 예시 1
val df1 = (1 to 100000).toList.toDF("col1")
df1.withColumn("col2",lit(2)).drop("col2").explain(true) // col2에 대한 처리는 생략된다.

// 예시 2
val arr = Array(1,2,3,4,5,6)
arr.filter(_<=4).filter(_%2==0).first 
// eager: [2,4]를 구한 후 2를 구함
// lazy: 첫번째 값만 구하면 되기때문에 2만 구함
```

## 3-2. 쿼리 엔진
SQL-on-Hadoop에 의한 데이터 처리의 구체적인 예  

### 데이터 마트 구축의 파이프라인
- 목표는 데이터 마트에서 `빠른 조회`
- 파이프라인  
    지연 시간이 긴 Hive로 데이터를 조회하기 빠른 형태로 구조화하고, 지연시간이 적은 Presto로 데이터를 조회하는 구조
    - `Hive`: 분산 스토리지 내 비구조화 데이터 → 구조화 데이터 → 열 지향 스토리지 형식으로 저장
    - `Presto`: 완성한 구조화 데이터를 결합, 집계하고 비정규화 테이블로 데이터 마트에 내보낸다

<img src="https://user-images.githubusercontent.com/40620421/186680026-d52e7605-9ea2-47a9-87e0-d35e199999ca.png" width="500">

<br/>
<br/>

#### Hive에 의한 구조화 데이터 만들기

```sql
-- 예시 1) 바로 조회
SELECT status, count(*) cnt FROM access_log_csv
-- 8.664 seconds (바로 집계는 비효율적)


-- 예시 2) 구조화 진행 후 조회
CREATE TABLE access_log_orc STORED AS ORC AS
SELECT cast(TIME as timestamp) time, request, status FROM access_log_csv
-- 15.993 seconds (시간이 걸리는 프로세스임으로 Hive와 같은 배치형 쿼리 엔진이 적합)

SELECT status, count(*) cnt FROM access_log_csv
-- 1.567 seconds (구조화 데이터를 집계하는 것이 훨씬 효율적)
```

Hive로 비정규화 테이블 작성  
- 데이터 마트를 구축하기 위해 비정규화 테이블을 작성한다
- 시간이 오래 걸리는 작업인 경우 지연시간이 총 작업시간에 큰 영향을 주지 않고, 리소스 이용 효율을 높일 수 있어 Hive를 활용하는 것이 원칙적이다

Hive 쿼리 개선  
1. 서브 쿼리 안에서 레코드 수 줄이기

    ```sql 
    -- 비효율적인 쿼리 (전체 데이터를 조회한 후 Filter)
    SELECT ... 
    FROM access_log a 
    JOIN users b ON b.id=a.user_id
    WHERE b.created_at = '2017-01-01'

    -- 보다 효율적인 쿼리 (초기에 팩트 테이블을 작게한다)
    SELECT ... 
    FROM (
        SELECT * access_log
        WHERE time >= TIMESTAMP '2017-01-01 00:00:00'
    ) a
    JOIN users b ON b.id=a.user_id
    WHERE b.created_at = '2017-01-01'
    ```

1. 데이터 편향 피하기

    ```sql
    -- 비효율적인 쿼리 (distinct count는 분산되지 않아 처리가 오래 걸림)
    SELECT date, count(distinct user_id) users
    FROM access_log GROUP BY date

    -- 보다 효율적인 쿼리 (최초에 중복을 없앤다)
    SELECT date, count(*) users
    FROM(
        SELECT distinct date, user_id FROM access_log
    ) a
    GROUP BY date
    ```

#### 대화형 쿼리 엔진 Presto의 구조 
- `대화형 쿼리 엔진`: 쿼리 실행 지연을 감소하여 작은 쿼리를 여러 번 실행하는 엔진
- BigQuery Impala, Presto 등

`Presto`  
- 플러그인 가능한 스토리지
    - Hive와 같이 하나의 쿼리에서 `여러 개의 데이터 소스 연결 가능` (전용 스토리지 X)
        - Hive의 메타 스토어를 활용할 수 있음 → Hive와 연동성이 좋다
        - 열 지향 데이터 구조(ORC)로 되어있을 때 최대 성능 → 따라서 Hive에서 구조화된 데이터를 가져옴
    - MPP DB의 경우, 스토리지와 컴퓨팅 노드가 밀접한 연관
- CPU 처리의 최적화
    - 쿼리를 분석하여 최적의 실행 계획을 생성 → 워크 노드에 배포 → 병렬로 처리
    - CPU 이용 효율이 높음 (CPU와 메모리 성능이 좋으면 최대 성능)
    - 실행이 시작되면 중간에 끼어들 수 없음 → `너무 큰 쿼리 적합하지 않음`
- 인 메모리 처리에 의한 고속화
    - Hive와 달리 쿼리 실행 과정에서 디스크에 쓰지 않음
    - 메모리가 부족하면 여유가 생길 때까지 기다리거나 오류
    - 너무 오래 걸리는 작업은 Hive에 맡기는 게 좋음
- 분산 결합과 브로드캐스트 결합  
    테이블 결합 시 조인키를 메모리 상에 유지하는 방법
    - 분산 결합: 같은 키를 갖는 데이터는 동일한 노드에 모임 → 노드 간에 데이터 전송이 쿼리 지연을 초래함
    - 브로드캐스트 결합: 한쪽 테이블이 작을 경우, 결합하는 테이블에 모든 데이터가 각 노드에 복사된다  
    <img src="https://user-images.githubusercontent.com/40620421/187230686-67a9ad4b-9954-4844-a251-5e96ba958357.png" width="350">
    <img src="https://user-images.githubusercontent.com/40620421/187230707-140fc800-3324-47d4-8b77-45a937e3b1e2.png" width="350">

### 데이터 분석의 프레임워크 선택하기
- MPP DB
    - 구조화 데이터를 SQL로 집계하는 것뿐이라면 좋은 선택
    - 스토리지와 계산 노드가 일체화 (확장성, 유연성이 안 좋음)
    - BI도구와 조합에서 오랜 실적이 있어 시각화용 데이터 마트에 구축에 좋음
- Hive
    - 높은 확장성, 내결함성(안정성), 대규모 배치처리
    - 지연시간이 길어서 무거운 처리에 적합
        - 열 지향 스토리지 생성, 텍스트 데이터 가공
- Presto
    - 속도가 빠른 대화형 쿼리 엔진
    - 안정성이 떨어짐
        - 메모리가 부족하면 쿼리실행 불가
        - 실행 중 장애가 나면 처음부터 다시 실행
    - 표준 SQL 준수, 데이터 분석을 위해 자주 사용하는 쿼리엔진
    - 다양한 데이터 스토어에 대응 (Hadoop, MySQL, Cassandra, MongoDB)
- Spark
    - 인메모리 데이터 처리가 중심
    - ETL 프로세스에서 SQL에 이르기 까지의 흐름을 하나의 데이터 파이프라인으로 기술할 수 있음
    - SparkSQL을 통해 주력-SQL로 활용할 수도 있음

## 3-3. 데이터 마트 구축
시각화를 위해 데이터 마트를 구축  

### 팩트 테이블
- 작은 팩트 테이블을 모두 메모리에 올릴 수 있지만, 아니라면 열 지향 스토리지로 변환 필요
    - 이러한 구조화 과정에 대상 1순위
- 작성 방법
    1. 추가: 새로 도착한 데이터만을 증분으로 추가
    1. 치환: 과거의 데이터를 포함하여 테이블 전체를 치환

#### 테이블 파티셔닝
- `논리적인 하나의 테이블`을 `여러 물리적인 파티션`으로 나눔으로써 정리할 수 있음
- 추가하는 것이 효율이 더 좋지만, 단점 존재 → 이러한 가능성을 줄임
    1. 추가 실패에서 알아채지 못 하면 결손이 발생
    1. 추가를 잘못해서 여러번 실행하면 중복
    1. 다시 팩트 테이블을 만들고 싶을 때 관리가 복잡해짐
- 1일 1회, 1시간 1회라는 식으로 자주 새 파티션을 만들고 그것을 팩트 테이블에 붙여 놓음
- 많은 데이터를 다룰 때 용이 → `DW를 구축하는데 유용`
- 장점
    1. Select 쿼리 성능 향상
    1. 디스크 장애 시 파티션만 영향을 받음으로 데이터 훼손 가능성이 감소
    1. 조인시 파티션 간의 병렬 처리 및 파티션 내 병렬 처리 가능
    1. 파티션 단위로 디스크 I/O를 분산해 부하 감소
- 단점
    1. 파티션 키 값 변경에 대한 관리 필요
    1. Insert 속도가 느려짐
    1. Join에 대한 비용 증가
    
#### 데이터 마트의 치환
- 테이블을 모두 다시 쓴다. (일일 보고서를 위해 지난 30일 동안 데이터를 매일 꺼내 치환)
- 데이터 양이 아주 많지 않은 `데이터 마트에 사용됨`
- 장점
    1. 데이터가 중복되거나 빠드릴 가능성이 거의 없음
    1. 스키마 변경 등에 유연하게 대응
    1. 오래된 데이터는 자동으로 지워져 데이터 마트가 계속 확대되는 일이 없음
- 단점
    1. 처리 시간이 오래걸림 
        - 데이터가 많다면 문제가 커짐
        - 데이터 처리가 1시간 이내에 완료되지 않는다면 추가+파티셔닝 고려

### 집계 테이블
- 팩트 테이블을 어느 정도 모아 집계하여 데이터 양을 줄인다
- 칼럼이 취하는 값의 범위를 `카디널리티`라고 함
    - 집계 테이블을 구성할 때 카디널리티를 줄여야함
    - 다만 너무 낮으면 정보 손실이 되는 것을 주의

### 마스터 테이블
- 팩트 테이블에서 참고되는 각종 정보
    - 상황에 따라 일부 데이터가 업데이트됨
    - 고객 테이블에서 고객ID는 변경되지 않지만, 고객 주소와 같은 속성은 변경될 수 있음
- 이를 기록하기 위한 방법
    1. 스냅샷 테이블
        - 테이블을 통째로 저장하는 방법
        - 취급이 쉬움
        - 스냅샷은 기간의 시작이 아니라, 끝에 저장되야함
    1. 이력 테이블
        - 변경 내용만을 저장하는 방법
        - 데이터 양을 줄이는 데 도움이 됨
        - 나중에 복원하기가 어려워짐

### 비정규화 테이블 완성
- 팩트 테이블과 디멘전 테이블을 결합하여 비정규화 테이블을 만든다
    - 디멘전 테이블: 스냅샷 뿐만 아니라 목적에 따라 각종 중간 테이블이 만들어짐
- 카디널리티가 너무 크면 시각화가 제대로 되지 않기에 신경 써야함

```sql
SELECT
    date_trunc('day', a.time) time -- 1일 단위 그룹화 (디멘전)
    date_diff('day', b.min_time, a.time) days -- 방문한 후 걸린 일 수 (추가 디멘전)
    count(*) cnt
FROM(
    SELECT time, session_id FROM access_log -- 팩트 테이블에서 필요한 칼럼만 추출
    WHERE time BETWEEN TIMESTAMP '2017-01-01' AND TIMESTAMP '2018-01-01' -- 집계 기간
) a
JOIN sessions b ON b.session_id=a.session_id -- 디멘전 테이블과 결합
GROUP BY 1, 2 -- 1, 2열을 기준으로 그룹화
```

<br/>
<br/>
<br/>


# 4. 빅데이터의 축적

## 4-1. 벌크형과 스티리밍형의 데이터 수집
각 방법으로 분산 스토리지에 데이터가 저장될 때까지 흐름을 살펴봄

### 객체 스토리지
- 빅데이터에서 대부분의 경우 확장성이 높고, 대량으로 파일을 저장하기 위해 `분산 스토리지``객체 스토리지`를 사용
- 파일 읽고, 쓰기는 네트워크를 거쳐서 실행 → 데이터가 늘어나도 성능이 떨어지지 않음
- 데이터는 여러 디스크에 복사되어 내결함성을 가짐
- 통신 오버헤드가 커서 소량의 데이터에서는 비효율적
- HDFS, Amazon S3

#### 데이터 수집
- 수집한 데이터를 가공하여 집계 효율이 좋은 분산 스토리지를 만드는 프로세스
- 작은 데이터는 모아서 하나로 만들고, 큰 데이터는 복수로 나누는 것을 고려
    - 대량의 작은 파일을 생성하는 것은 성능 저하
    - 파일 크기가 너무 증가하면 네트워크 전송에 시간이 걸려 예상치 못한 오류 발생

### 벌크형의 데이터 전송
- DB, 파일서버, 웹 서버 등에서 각각의 방식으로 정리해 데이터 추출
- 전통적인 DW에서 사용 
- 신뢰성이 좋음 (실패할 경우 나중에 재실행하기 쉬움)
- 데이터 전송을 위해 `ETL 서버`를 설치
    - ETL 서버에서 데이터를 구조화하여 DW에 전송한다
    - 정기적인 실행(1시간 or 하루)을 하여 그동안 축적된 데이터를 하나로 모은다 (데이터의 크기가 적당하도록 주기를 설정하는 것이 중요)
- 워크 플로 관리 도구와 조합이 좋음


<img src="https://user-images.githubusercontent.com/40620421/187693026-3d0ab85a-aaf2-4e77-b268-ed654cef1f25.png" width="500">


### 스트리밍형의 데이터 전송
- 계속해서 전송되어 오는 작은 데이터를 취급
    - 바로 생성되어 어디에도 저장되지 않는 데이터는 그 자리에서 바로 전송됨
    - 웹 브라우저, 모바일 앱, 센서 기기 등 
- `메시지 배송`: 다수의 클라이언트에서 데이터를 전송받는 방식
    - 데이터 양에 비해 통신 오버헤드가 커서 높은 수준의 서버 필요
    - 작은 데이터 쓰기에 적합한 No SQL 활용
    - `메시지 큐`, `메시지 브로커` 등의 중계 시스템을 활용하여 일정한 간격으로 꺼내고 모아서 함께 분산 스토리지에 저장

<img src="https://user-images.githubusercontent.com/40620421/187695073-50818484-b452-4bfa-9804-bd7cac5890a6.png" width="500">

## 4-2. 메시지 배송의 트레이드 오프 (성능X신뢰성)
- 클라이언트 수가 많아지면 스트리밍 형의 메시지 배송의 `성능`과 `신뢰성`을 둘 다 만족하기가 어려워짐

### 메시지 브로커
- 스토리지의 성능 문제를 해결하는 중간층 설치
- 기존 문제점
    1. 외부에서 오는 메시지 양을 제어할 수 없음
    1. 쓰기 빈도가 계속해서 증가하여 오류가 날 수 있음
    1. 오류가 발생하면 클라이언트에서 재전송을 하려고 함 (악순환)
- 해결방법
    1. 좋은 성능을 가진 분산 스토리지
    1. 데이터를 중간에서 일시적으로 축척하는 메시지 브로커 설치
- 생산자와 소비자
    - `생산자`가 메시지 브로커에 데이터를 보냄
    - `소비자`가 메시지 브로커에서 데이터를 꺼냄
- `메시지 라우팅`
    - 데이터가 복수의 다른 소비자에서 읽어들일 수 있음 
    - 메시지가 복사되어 데이터를 여러 경로로 분기시킴
- Apache Kafka, AWS Kinesis

### 신뢰성 있는 메시지 배송
- 메시지의 중복이나 누락이 발생한 경우 3가지 중 하나를 보장하도록 설계
1. `at most once`
    - 메시지는 한 번만 전송됨 (메시지를 절대 다시 보내지 않음)
    - 전송에 실패되어 사라질 가능성이 있음 (결손 발생)
    - 오류를 감지하더라도 "메시지가 보내지지 않았다"를 증명하기 어려울 때
        - 수신측에서는 데이터를 받았으나, ack가 반환되는데 에러가 나타난 경우, 다시 보내면 데이터가 중복된다
1. `exactly once`
    - 네트워크상에 분단된 노드 사이에 `코디네이터`를 통해서 통신 내용을 보장받을 수 있을 때 활용
    - 장점: 코디네이터가 문제를 해결하여 메시지를 한번만 보낼 수 있음
    - 단점: 코디네이터에 의존하게 되고, 시간이 많이 소요
1. `at least once`
    - 메시지가 재전송되어도 그것을 없앨 수 있는 구조라면 활용
    - 시퀀스 번호를 통해 같은 번호가 중복해서 오면 파기한다. (중복 제거)
    - 대부분의 메시지 브로커에서 활용

#### 중복 데이터 핸들링 방법
1. 오프셋을 이용한 중복 제거
    - 전송해야할 데이터에 파일명과 시작위치(`오프셋`)을 덧붙인다
    - 데이터가 중복되더라도 같은 장소에 덮어써진다
    - 벌크형 데이터 전송과 같이 데이터 양이 고정된 경우 잘 작동
1. 고유 ID에 의한 중복 제거
    - 스티리밍형의 메시지 배송에서 자주 사용
    - ID가 폭발적으로 늘어난다 → 최근에 받은 ID만 기억한다 (중복은 대부분 일시적인 통신 오류 때문임으로)
1. 종단 간의 신뢰성
    - 중간 경로를 모두 `at least once`로 통일하고, 모든 메시지에 고유 ID를 포함하도록 한다
    - 경로의 말단에서 중복 제거를 실행
1. 고유 ID를 사용한 중복 제거의 방법
    - NoSQL: 특성상 데이터를 쓸 때 고유ID를 지정한다. ID가 겹치면 덮어쓴다.
    - SQL: 객체 스토리지에 우선 저장한 뒤 대규모 데이터 처리(Hive)를 통해 중복 제거.

#### 데이터 수집 파이프라인
- 클라이언트에서 발생한 데이터를 분산 스토리지에 적합한 형태로 저장하는 일련의 과정
- 필요에 따라 시스템을 조합
    - 쓰기 성능에 불안함이 없다면 메시지 브로커 생략 가능
    - 다소 중복이 허용된다면 중복 제거 생략 가능
- 빅데이터 시스템에서는 매우 높은 성능이 중요함으로 아주 작은 중복은 무시하는 경향이 있다
    - 신뢰성이 중시되는 경우에슨 스트리밍형보다는 벌크형으로 데이터를 보내는 것이 좋다

## 4-3. 시계열 데이터의 최적화

