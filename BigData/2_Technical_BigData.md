2_Technical_BigData

<p align="center">
<img src="https://user-images.githubusercontent.com/40620421/183598835-ebf8d4ba-354d-4b02-8ccd-a7d1f2d29018.jpg" width="300">

본 내용은 해당 도서를 참고하여 정리한 내용입니다.
</p>

<br/>
<br/>
<br/>

# 목차
- [2. 빅데이터의 탐색](#2-빅데이터의-탐색)
  * [2-1. 크로스 집계의 기본](#2-1-크로스-집계의-기본)
  * [2-2. 열 지향 스토리지에 의한 고속화](#2-2-열-지향-스토리지에-의한-고속화)
    + [DB 지연 줄이기](#db-지연-줄이기)
    + [열 지향 DB 접근](#열-지향-db-접근)
      - [행 지향 DB](#행-지향-db)
      - [Index](#index)
      - [열 지향 DB](#열-지향-db)
  * [2-3. 애드 혹 분석과 시각화 도구](#2-3-애드-혹-분석과-시각화-도구)
  * [2-4. 데이터 마트의 기본 구조](#2-4-데이터-마트의-기본-구조)
    + [데이터 마트 구축](#데이터-마트-구축)
    + [테이블을 비정규화하기](#테이블을-비정규화하기)
- [3. 빅데이터의 분산 처리](#3-빅데이터의-분산-처리)
  * [3-1. 대규모 분산 처리 프레임워크](#3-1-대규모-분산-처리-프레임워크)
    + [데이터 구조화](#데이터-구조화)
    + [Hadoop](#hadoop)
    + [HDFS (Hadoop Distributed File System)](#hdfs-hadoop-distributed-file-system)
    + [YARN (Yet Another Resource Negotiator)](#yarn-yet-another-resource-negotiator-)
    + [ZooKeeper](#zookeeper)
    + [Hive](#hive)
      - [👉 메타 스토어](#👉-메타-스토어)
      - [👉 아키텍처](#👉-아키텍처)
      - [👉 데이터 저장 방법](#👉-데이터-저장-방법)
      - [Impala, Presto](#impala-presto)
    + [MapReduce](#mapreduce)
    + [Spark](#spark)
      - [👉 RDD vs DataFrame vs Dataset](#👉-rdd-vs-dataframe-vs-dataset)
      - [👉 DAG](#👉-dag)
      - [👉 Spark 연산 과정](#👉-spark-연산-과정)
      - [👉 Lazy Evaluation](#👉-lazy-evaluation)
  * [3-2. 쿼리 엔진](#3-2-쿼리-엔진)
    + [데이터 마트 구축의 파이프라인](#데이터-마트-구축의-파이프라인)
      - [Hive에 의한 구조화 데이터 만들기](#hive에-의한-구조화-데이터-만들기)
      - [대화형 쿼리 엔진 Presto의 구조](#대화형-쿼리-엔진-presto의-구조)
    + [데이터 분석의 프레임워크 선택하기](#데이터-분석의-프레임워크-선택하기)
  * [3-3. 데이터 마트 구축](#3-3-데이터-마트-구축)
    + [팩트 테이블](#팩트-테이블)
    + [집계 테이블](#집계-테이블)
    + [마스터 테이블](#마스터-테이블)
    + [비정규화 테이블 완성](#비정규화-테이블-완성)
- [4. 빅데이터의 축적](#4-빅데이터의-축적)
  * [4-1. 벌크형과 스티리밍형의 데이터 수집](#4-1-벌크형과-스티리밍형의-데이터-수집)
    + [객체 스토리지](#객체-스토리지)
    + [벌크형의 데이터 전송](#벌크형의-데이터-전송)
    + [스트리밍형의 데이터 전송](#스트리밍)
  * [4-2. 메시지 배송의 트레이드 오프 (성능X신뢰성)](#4-2-메시지-배송의-트레이드-오프-성능x신뢰성)
    + [메시지 브로커](#메시지-브로커)
    + [신뢰성 있는 메시지 배송](#신뢰성-있는-메시지-배송)
  * [4-3. 시계열 데이터의 최적화](#4-3-시계열-데이터의-최적화)
    + [프로세스 시간에 의한 분할](#프로세스-시간에-의한-분할)
    + [이벤트 시간에 의한 분할](#이벤트-시간에-의한-분할)
  * [4-4. 비구조화 데이터의 분산 스토리지](#4-4-비구조화-데이터의-분산-스토리지)
    + [NoSQL DB 활용](#nosql-db-활용)
    + [ACID와 CAP](#acid와-cap)
    + [분산 Key-Value Store](#분산-key-value-store)
    + [와이드 Column Store](#와이드-column-store)
    + [도큐먼트 스토어](#도큐먼트-스토어)
    + [검색 엔진](#검색-엔진)
- [5. 빅데이터의 파이프라인](#5-빅데이터의-파이프라인)
  * [5-1. 워크플로 관리](#5-1-워크플로-관리)
    + [워크플로 관리 도구](#워크플로-관리-도구)
    + [멱등한 조작으로 태스크 작성](#멱등한-조작으로-태스크-작성)
  * [5-2. 배치 형의 데이터 플로우](#5-2-배치-형의-데이터-플로우)
  * [5-3. 스트리밍 형의 데이터 플로우](#5-3-스트리밍-형의-데이터-플로우)
    + [스트림 처리 결과를 배치 처리로 치환하기](#스트림-처리-결과를-배치-처리로-치환하기)
      - [람다 아키텍처](#람다-아키텍처)
      - [카파 아키텍처](#카파-아키텍처)
    + [아웃 오버 오더 데이터 처리](#아웃-오버-오더-데이터-처리)

<br/>
<br/>
<br/>

# 2. 빅데이터의 탐색

## 2-1. 크로스 집계의 기본
- 데이터 시각화에서 먼저 기본이 되는 것이 크로스 집계
- `크로스 집계`: 트랜잭션 테이블에서 크로스 테이블로 변환하는 과정
    1. 크로스 테이블: 행과 열이 교차하는 부분에 숫자 데이터가 들어가는 테이블
    1. 트랜잭션 테이블: 행을 기준으로 데이터가 추가되는 테이블

<img src="https://user-images.githubusercontent.com/40620421/185924635-c1f3f12d-b655-49ee-9382-6ba4b359acf0.png" width="500">

### SQL에 의한 테이블 집계
- 대량의 데이터를 크로스 집계하기 위해 SQL을 활용
    - 데이터 양이 너무 많으면 피벗 테이블을 통한 크로스 집계가 어려움
- SQL을 통해서 크로스 테이블이 아닌 트랜잭션 테이블의 형태로 얻고, 이를 크로스 집계함으로써 임의의 크로스 테이블을 얻을 수 있음
```sql
SELECT 매출일, 점포 ID, 금액 FROM 판매이력
```
매출일|점포ID|고객ID|금액
--|--|--|--
2022-08-01|1|10|60000
2022-08-01|1|11|50000
2022-08-01|2|12|30000
2022-08-02|1|13|120000
2022-08-02|2|14|50000
2022-08-02|2|15|40000

점포ID\매출일|2022-08-01|2022-08-02
--|--|--
1|110000|120000
2|30000|90000

- 데이터 집계를 `SQL`로 하고, 크로스 집계를 `시각화 도구`로 함으로써 이론상 무한히 많은 데이터가 있더라도 크로스 집계가 가능하다
    - `데이터 집계의 프로세스`: SQL로 집계
    - `시각화 프로세스`: 시각화 도구로 크로스 집계

## 2-2. 열 지향 스토리지에 의한 고속화
대량의 데이터를 신속하게 집계하기 위한 DB 구조



### DB 지연 줄이기
- RDB는 메모리가 부족하면 급격히 성능이 저하된다
- `MPP 기술`
    - 고속화를 위해 사용되는 기법이 `압축과 분산`
    - 데이터를 최대한 압축한 후, 여러 디스크에 분산함으로써 데이터 로드의 지연을 줄인다
    - 멀티 코어를 활용하여 디스크 I/O를 병렬처리
    - 하드웨어 수준에서 균형된 CPU와 디스크를 가진 DB를 `MPP 데이터베이스`라고 함
    - Amazon RedShift, Google BigQuery

### 열 지향 DB 접근

#### 행 지향 DB
- 일반적으로 많이 사용되는 DB
- 데이터 검색을 위해서는 모든 레코드를 가져와야함 → 성능 저하
- `Index`를 통해 데이터 검색을 고속화
- 하지만 데이터 분석에서는 어떤 칼럼이 사용되는지 미리 알 수 없기 때문에 Index가 큰 도움이 되지 않음

#### Index
- 장점
    - 테이블을 조회하는 속도와 성능 향상
    - 시스템 부하를 줄임
- 단점
    - 인덱스를 관리하기 위한 추가 저장 공간 필요
    - 인덱스를 항상 정렬된 상태로 유지하기에 추가 작업이 필요
    - 인덱스를 잘못 사용할 경우 역효과가 날 수 있다.
- Index 권장 케이스
    - 데이터 양이 많은 테이블
    - 업데이트보다 조회가 잦은 테이블
    - 조건문이나 정렬이 잦은 테이블
- Index 자료 구조
    - Hash 인덱스 알고리즘, B-Tree 인덱스 알고리즘, B+Tree 인덱스 알고리즘 (균형 트리)

#### 열 지향 DB
- 데이터를 미리 `칼럼 단위`로 정리해 둠으로써 필요한 칼럼만 로드하여 디스크 I/O를 줄임
- 일부 칼럼만이 집계 대상이 될 때 유용함
- `압축 효율`이 우수함 (같은 칼럼에는 유사한 데이터가 나열되기 때문)
- `MPP`를 통해 하나의 쿼리를 다수의 작은 태스크로 분해하고 병렬로 실행
    - 디스크로부터 로드가 병목 현상이 발생하지 않도록 데이터가 고르게 분산되어 있어야함

## 2-3. 애드 혹 분석과 시각화 도구

### 대시보드 도구
- 새로운 그래프를 쉽게 추가할 수 있는 도구
- 최신의 집계 결과를 즉시 확인할 수 있길 기대
- 정해진 지표의 일상적인 변화를 모니터링

### BI 도구
- 대화형 데이터 탐색이 중요시됨
- ex. 몇개월 단위의 장기적인 데이터 추이를 시각화, 집계의 조건을 세부적으로 바꿈
- Tableau

## 2-4. 데이터 마트의 기본 구조
BI 도구에서 대화형으로 데이터를 참고하려면, 시각화 정보만을 모은 데이터 마트가 필수

### 데이터 마트 구축
- `OLAP`/online analytical processing
    - 사용자가 다차원 정보에 접근하여 대화식으로 정보를 분석하도록 지원
- `다차원 모델`
    - 데이터 집계를 효율화하는 접근 방법 중 하나
    - 데이터 분석을 위해 만들어진 다차원 데이터를 `OLAP 큐브`라고 하고, 그것을 크로스 집계하는 구조가 `OLAP`
    - 크로스 집계의 모든 조합을 미리 계산하여 DB안에 캐시해두고, 쿼리가 실행되면 집계된 결과를 반환하는 구조
    - 데이터 마트가 이전에 가지고 있던 구조
- `MPP DB와 비정규화 테이블`
    - MPP DB와 인 메모리 DB 등의 보급으로 사전에 계산할 필요가 없음
    - MPP DB에 다차원 모델의 개념이 없기 때문에 비정규화 테이블을 준비
    - 시각화에 적합한 데이터 마트를 만드는 것은 BI 도구를 윈한 `비정규화 테이블`을 만드는 프로세스

### 테이블을 비정규화하기

#### 정규화 테이블
- `트랜잭션 테이블`: 시간과 함께 생성되는 데이터
    - 시간과 함께 생성되기에 변하지 않음
- `마스터 테이블`: 트랜잭션에서 참고되는 각종 정보
    - 상황에 따라 일부 데이터가 업데이트됨
    - 고객 테이블에서 고객ID는 변경되지 않지만, 고객 주소와 같은 속성은 변경될 수 있음

#### 비정규화 테이블
- 과거
    - DW에서는 트랜잭션과 마스터를 `팩트 테이블`과 `디멘젼 테이블`이라고 칭함
    - 데이터 마트를 만들 때 팩트 테이블을 중심으로 여러 디멘젼 테이블을 결합하는 것이 좋음 (`스타 스키마`)
- 현재
    - MPP DB와 열 지향 스토리지 시스템의 발달로 칼럼수가 아무리 늘어나도 성능에 영향을 주지 않는다
    - 처음부터 팩트 테이블에 모든 칼럼을 포함해두고, 쿼리 실행 시 테이블 결합을 하지 않는다
    - 장점
        1. 빠른 데이터 조회 (Join 비용이 줄어들기 때문)
        1. 데이터 조회 쿼리의 간단화
    - 단점
        1. 데이터 갱신이나 삽입 비용이 높음
        1. 데이터 무결성을 해침
        1. 데이터 중복 저장으로 인한 추가 저장공간 확보

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbJUNWP%2FbtqE2twgz5B%2FZ9qC1T74uCQ1CyAbKnUG60%2Fimg.png" width="500">

<br/>
<br/>
<br/>


# 3. 빅데이터의 분산 처리

## 3-1. 대규모 분산 처리 프레임워크
다수의 컴퓨터에 데이터 처리를 분산하기 위해서는 그 실행을 관리하는 프레임 워크가 필요하다.  
`Hadoop`, `Spark` 중심

### 데이터 구조화

<img src="https://user-images.githubusercontent.com/40620421/186179295-30d91728-6b4a-4246-bc20-d3c544b77397.png" width="500">

- `구조화 데이터`: 스키마가 명확하게 정의된 데이터
    - 스키마: 테이블의 칼럼 명, 데이터 타입, 테이블 간의 관계 등을 정의
    - 기존 DW에서 데이터를 축척하는 일반적인 방식
- `비구조화 데이터`: 스키마가 없는 데이터
    - 자연 언어로 작성된 텍스트 데이터, 이미지, 동영상 등
    - 이 상태로는 SQL로 제대로 집계할 수 없음
    - 이 데이터를 분산 스토리지 등에 저장하고 그것을 분산 시스템에서 처리하는 것이 `데이터 레이크`의 개념
- `스키마리스 데이터`: 기본 서식은 있지만, 스키마가 정의되지 않음
    - CSV, JSON, XML 등
    - 칼럼 수나 데이터 타입은 명확하지 않음
    - NoSQL DB에서 활용

#### 열 지향 스토리지
- `MPP DB`: 제품에 따라 스토리지 형식이 고정되어 사용자가 상세히 몰라도됨
- `Hadoop`: 직접 열 지향 스토리지 형식을 선택
    - `ORC`: 구조화 데이터를 위한 열 지향 스토리지. 처음 스키마를 정한 후 데이터 저장.
    - `Parquet`: 스키마리스에 가까운 데이터 구조. JSON과 같은 데이터도 그대로 저장 가능.

### Hadoop
- 대규모 분산시스템을 구축하기 위한 공통 플랫폼
- 분산 시스템을 구성하는 다수의 소프트웨어로 이루어진 집합체 (단일 소프트 웨어 X)
- Why Hadoop? 🤔
    1. 접근성과 비용 절감  
    접근성이 뛰어나서 유연한 방식으로 하드웨어를 사용할 수 있음 → 고가의 신뢰도 높은 하드웨어 만을 추구하지 않음 → 비용 절감
    기존 DB는 소프트웨어와 하드웨어가 비싸다
        
    2. 빅데이터에 대한 내결함성  
    기본적으로 HDFS는 파일을 `3군데 저장 `
    → 공간적으로 비효율적이다? 만약 한 데이터 노드에서 데이터를 구성할 수 있다면? 성능적 이득을 볼 수 있음
    → 왜 3개? 내결함성+성능 실험적으로 알맞음
        
    3. 확장성  
    하드웨어를 추가했을 때, 성능이 리니어하게 증가한다.
        
    4. 읽기 시점 스키마  
    기존에 쓰기 시점 스키마에서 벗어나, 하둡이나 NoSQL은 데이터를 읽을 때 데이터의 본질을 파악한다.
    데이터를 소비자에게 데이터의 성질을 맡기는 시스템. (데이터로 뭘하기 전까진 데이터로 무엇을 할지 정확히 알 수 없다.)
- Why Java? 🤔
    1. Java의 GC가 가장 성능이 좋음 (빅데이터를 다루기에 메모리 관점 중요)
    2. Java가 디버깅이 쉬움
    3. Hadoop이 Nutch 프로그램에서 발전 되었는데, 그것이 Java 기반
- 하둡 2 vs 3
    - 이레이저코딩 도입하여 기존의 블록 복제를 대체하는 방식   
        ⇒ 1GB 저장: 3GB → 1.5GB  
        ⇒ 데이터 손실 시 복구할 수 있는 기법. 데이터 복제를 대체하지는 않음
    - 2개 이상의 네임노드를 지원

참고  
https://intrepidgeeks.com/tutorial/hedou-you-should-know  
https://deep-flame.tistory.com/9

### HDFS (Hadoop Distributed File System)
- 데이터를 블록으로 나누어 저장하는 `분산 파일 시스템`
- 수정, 삭제 불가 → 한 번쓰고, 여러 번 읽는 구조에 적합함
- 마스터-워커 패턴 (Name Node가 쇼를 하고, Data Node가 모든 일을 한다)
    - `Name Node`
        메타 데이터 (모든 파일과 디렉토리에 대한 정보)
        Name Node가 없다면 Data Node에 있는 데이터를 불러올 수 없음
        따라서 Name Node의 정보를 지속적으로 백업

    - `Data Node`
        클라이언트나 네임노드의 요청에 블록을 저장하고, 탐색
- `HDFS의 기본 블록 사이즈 128MB` 왜 이렇게 큼?
    - 탐색 비용 최소화: 블록의 시작점을 탐색하는 데에 걸리는 시간을 줄일 수 있음

    - 전송 비용 최소화: 한 데이터 노드에서 데이터를 모두 구성할 수 있다면 데이터 블락을 한데로 모으는 시간을 최소화할 수 있음

    - 대용량 데이터를 기반으로 함

<img src="https://blog.kakaocdn.net/dn/baTHVO/btrqsdMTHNX/bChgeqd831xlqEc1naDzR0/img.gif" width="500">

```
Write-1. 데이터를 블록으로 나눈다.
Write-2. 블록을 저장하려는 클라이언트는 네임노드를 통해 블록이 저장될 데이터 노드의 목록을 받는다.
Write-3. 클라이언트는 첫 번째 데이터노드에 블록을 쓰고, 차례대로 다음 데이터노드로 파이프라인을 통해 데이터를 흘려보낸다. (네임노드는 데이터노드의 가용 저장 공간을 고려하여 파이프라인을 구성)
Read-1. 네임노드를 통해 읽을 블록을 가지고 있는 데이터 노드의 목록을 받는다.
Read-2. 최대한 가까운 곳에 위치한 데이터노드로부터 데이터를 읽어들인다.
```

### YARN (Yet Another Resource Negotiator)
- CPU나 메모리 등의 계산 리소스를 관리하는 시스템
- 리소스 매니저와 노드 메니지가 서로 통신하며 통해 수많은 워크 노드의 리소스를 관리
    - `리소스 매니저`  
        실행되는 애플리케이션을 예약하는 중앙 관리 시스템
    - `노드 매니저`  
        컨테이너를 시작하고 리소스 사용량(CPU, 메모리, 디스크, 네트워크)를 모니터링하고 리소스 매니저에 보고

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBN83W%2FbtrqnklKtxC%2FRLCe086rprV5phLHIIruFK%2Fimg.png" width="500">

```
Assign-1. 클라이언트가 리소스 매니저에 어플리케이션을 제출한다.
Assign-2. 워커 노드 내 어플리케이션 마스터는 리소스 매니저에게 어플리케이션을 실행할 리소스 할당을 요청한다. 
Assign-3. 이때 연산 수행에 필요한 자원은 서로 다른 컨테이너 단위로 분할되어 어플리케이션 마스터에게 전달된다.

Run-1. 어플리케이션 마스터는 노드 메니저에게 컨테이너의 실행 명령을 전달한다.
Run-2. 코드가 컨테이너에서 실행된다.
Run-3. 어플리케이션이 종료되면 어플리케이션 마스터는 리소스메니저에서 자신을 제거하고 셧다운된다.
```

### ZooKeeper
- Hadoop의 분산처리 환경에서 `조율을 관리하는 코디네이션 서비스`
- 복수의 컴퓨터가 네트워크를 통해 통신하며 하나의 목적을 위해 서로간에 상호작용한다. 이때 마치 하나인 것처럼 동작하는 시스템처럼 합의를 이끌어내는 서비스
- 고가용성 확보, 태스크 조율, 상태 추적, 일반적인 설정 파라미터 값 지정 수행

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkPeUK%2FbtrKvEx2xrY%2FPfX6CkmkvXsuIuRhETmKNk%2Fimg.png" >

아키텍처
1. 클라이언트가 주키퍼 서버에 데이터를 업데이트한다. (Client B > Server D)
2. Leader 서버에서 이를 알린다.
3. Leader 서버에서는 Broadcast 형식으로 Follower 서버들에게 알린다.
4. 모든 서버에서 데이터가 일관된 상태로 유지된다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F1jbGE%2FbtrKsO2SjhG%2FLknJjwgxd0kAVJRWwCRLZk%2Fimg.jpg">

`znode`
- 데이터를 저장하기 위해 사용하는 가장 작은 단위의 데이터 저장 객체
- 모든 데이터가 메모리에 저장되며, 최대 1MB로 제한적 → `설정 값이나 리소스 상태` 등을 저장하는 매우 유용
- Zookeeper는 여러 서버에 분산되어 있는 znode를 관리한다.
- 종류
    - Persistent Node: 명시적으로 삭제되기 전까지 존재.
    - Ephemeral Node: 세션이 유지되는 동안 존재. 자식 노드를 가질 수 없다

### Hive
- SQL 등의 쿼리 언어에 의한 데이터 집계 가능 (SQL-on-Hadoop)
    - 데이터가 저장된 HDFS에 접근하기 어려움
- 쿼리를 자동으로 MapReduce 프로그램으로 변환하는 소프트웨어로 개발됨 (Spark도 지원되긴 함)  
    → MapReduce가 대량의 배치 처리를 위한 시스템이다.  
    → `초기 지연이 너무 크기` 때문에 작은 쿼리 실행에는 적합하지 않다.  
    → `배치 처리`에 적합  
- Hive는 DB가 아닌 데이터 처리를 위한 배치 처리 구조이다. (쿼리 성능 고민 필요)

#### 👉 메타 스토어
- HDFS에 적재된 데이터의 메타정보(파일 위치, 이름 등)을 Table Schema 정보와 함께 메타스토어에 등록  
→ Hive 쿼리를 수행할 때 메타스토어의 정보를 참조하여 마치 RDBMS에서 데이터를 조회하는 것 같은 기능 제공

#### 👉 아키텍처

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMBATj%2FbtrKwbh45zy%2FD0q0JwtPs5MKOUFCPbKG41%2Fimg.png" width="500">

하이브는 사용자 쿼리를 파싱하고, `최적화`해서 하나 이상의 연쇄 배치 연산으로 `컴파일`하며 이를 클러스터에서 실행한다. 
1. HiveQL 문을 Driver가 받고, 메타스토어의 정보를 활용하여 적합한 형태로 컴파일
2. 컴파일된 SQL을 실행 엔진으로 실행
3. 리소스 매니저가 클러스터 자원을 적절히 활용하여 실행
4. 원천 데이터는 HDFS를 활용
5. 결과를 사용자에게 변환 

#### 👉 데이터 저장 방법
- **SerDe**: Hive가 데이터를 해석하는 방법을 제공
    - Avro, ORC, RegEx, Thrift, Parquet, CSV, JSONSerDe
    - Parquet: 열 기반 데이터 구조 (인코딩 효율이 높음, 쿼리 성능이 높음)
- 쿼리 성능 향상을 위한 데이터 형식
    - **Partition**
        - Column 정보를 이용하여 폴더 단위로 데이터가 생성 (큰 데이터를 작은 데이터로 쪼갬)
        - 기본적으로 테이블의 모든 row를 읽음 (파티션이 있다면 폴더의 데이터만 읽어 성능 향상)
        - Partition 범주가 너무 많지 않도록 주의
    - **Bucket**
        - 지정된 칼럼의 값을 해쉬 처리하고, 데이터를 지정한 수의 파일로 나누어 저장
        - 범주가 40이고, 버켓이 20이면 한 버켓이 2개의 범주씩 쌓인다
        - 조인 키로 버킷을 생성해두면 생성된 버킷 중 필요한 버킷만 조회하면 됨 → 성능 향상
    - **Skew**
        - 주로 많이 들어오는 데이터가 몰릴 때 사용 (A, B 외 나머지 총 3개의 디렉토리나 파일로 구별)
        - 네임노드의 관리 포인트가 줄어든다.


#### Impala, Presto
- 대화형의 쿼리 실행 전문
- 초기 지연이 적음 → Hive를 통해서 구조화 데이터를 만들고, 뒤에 활용됨

👉 **Impala**
1. SQL, HiveQL 모두 사용 가능
2. HDFS에 저장 + Hive 메타스토어를 사용
3. C++을 통해 구현하여 속도 개선 + 실시간 데이터 처리 + `데이터 소비자` 
4. 새로 프로젝트를 시작한다면 좋은 옵션

👉 **Presto**
1. 페타바이트 급의 데이터처리 SQL 사용
2. 대기 시간에 최적화, 쿼리 처리량 메모리 양에 제한 + `데이터 소비자` 
3. 기존의 진행 중인 데이터 시스템을 수정할 필요없이 기존의 생태계와 원활하게 통합되도록 설계
4. 컴퓨팅과 스토리지가 별도로 수행되어서 클라우드 환경에 적합하다.

### MapReduce
- 분산 시스템에서 데이터 처리를 하는 시스템
- 비구조화 데이터를 가공하는 데 적합
- `Map Task`: 분할된 레코드를 Key-Value형태로 변환한 결과를 반환한다.
- `Reduce Task`: 하나의 키에 대한 여러 값의 집계 또는 결합해서 입력값의 개수보다 더 작은 개수의 결과값을 산출한다.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbhfcnK%2FbtrqsFwXfmJ%2F46fgAZoKuSSWyViDgDMVMK%2Fimg.png" width="500">

```
예) 단어 개수 세기
Map. (단어, 1)의 Key-Value 구조의 리스트를 반환한다.
Shuffling. 단어 중심으로 데이터를 데이터를 모은다.
Reduce. (단어, count)를 수행하여 각 블록에서 특정 단어가 몇 번 나왔는지 계산한다.
```

### Spark
- MapRduce의 단점을 극복하기 위한 데이터 처리 시스템
    - Map Reduce의 단점
        - 맵리듀스로 복잡한 파이프라인을 조합하는 것은 많은 분석가들에게 부담
        - 많은 양의 디스크 기반 I/O를 수행한다. 따라서 다중 단계 파이프라인은 I/O 비용이 매우 많이 든다.
- 대량의 `메모리를 활용`하여 고속화를 실행
    - 가능한 많은 데이터를 메모리상에 올린 상태로 두어 디스크에는 아무것도 기록하지 않는다.
    - 컴퓨터가 비정상 종료하는 경우에도, 그때 처리를 다시 시도해서 중간 데이터를 다시 생성하면 된다.
- 특징
    - JDK 필요. Spark SQL 이용
    - 다양한 스크립트 언어 지원(Java, Scala, Python, R 등)
    - MapReduce 개선 사항으로 설계됨 → Hadoop과 호환성이 좋음

#### 👉 RDD vs DataFrame vs Dataset
- `RDD (Resilient Distributed Datasets)`
    - 스키마가 없는 분산된 데이터 모음 (스키마 수동 정의)
    - 메모리 내부에서 데이터 손실하더라도 재연산으로 복구할 수 있는 데이터 집합 → 내결함성 방식
    - `Lineage(DAG)` + 읽기전용 → 데이터 처리의 관계성을 명확히 할 수 있음 → 특정 RDD에서 메모리가 유실되더라도 복기할 수 있음
- `DataFrame`
    - 분산된 데이터를 Column으로 모은 데이터 구조
    - 관계형 테이블과 같은 구조 → 대용량 데이터를 좀 더 쉽게 처리
    - 스키마 자동 정의
- `DataSet`
    - DataFrame에서의 확장 (Dataset[Row] == DataFrame)
    - Scala and Java에서만 활용가능
    - 스키마 자동 정의

#### 👉 DAG
- 방향성 비순환 그래프
    - 노드와 노드가 화살표로 연결된다. (방향성)
    - 화살표를 아무리 따라가도 동일 노드로는 돌아오지 않는다.
- 장점
    - 태스크들의 종속성과 순서를 명확히 구성할 수 있음
    - 워크 플로우에서 노드가 중복으로 수행되면 데이터 중복이 발생할 수 있음. DAG은 이를 방지할 수 있음

#### 👉 Spark 연산 과정
- Logical Plan
    연산 단계에서 사용될 DataFrame이나 Column이 실제로 존재하는지에 대한 검사를 시행한다.
- Physical Plan
    1. 클러스터에서 Logical Plan을 어떻게 실행할지 정의한다. 
    2. 그 방법은 다양한 방법을 시도해보고, Cost Model을 이용하여 비교한다. 
    ⇒ `Best Physical Plan`을 선택

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbOYdlK%2Fbtrq7ao3mSU%2FLbu13eBKp3vQuPfkamwjU0%2Fimg.jpg">

#### 👉 Lazy Evaluation
`Action이 시작되는 시점`에 Transformation끼리 연계를 파악해 실행 계획을 최적화 (physical plan)
- Transformation: 새로운 RDD를 생성하는 동작 (map, filter, distinct)
- Action: 기록된 모든 작업을 실제로 수행하는 연산 (first, show, collect, count)

<img src="https://user-images.githubusercontent.com/40620421/186442906-7503b6ce-e867-43c2-9fa8-17fba7b7d9c7.png" width="500">

```scala
// 예시 1
val df1 = (1 to 100000).toList.toDF("col1")
df1.withColumn("col2",lit(2)).drop("col2").explain(true) // col2에 대한 처리는 생략된다.

// 예시 2
val arr = Array(1,2,3,4,5,6)
arr.filter(_<=4).filter(_%2==0).first 
// eager: [2,4]를 구한 후 2를 구함
// lazy: 첫번째 값만 구하면 되기때문에 2만 구함
```

## 3-2. 쿼리 엔진
SQL-on-Hadoop에 의한 데이터 처리의 구체적인 예  

### 데이터 마트 구축의 파이프라인
- 목표는 데이터 마트에서 `빠른 조회`
- 파이프라인  
    지연 시간이 긴 Hive로 데이터를 조회하기 빠른 형태로 구조화하고, 지연시간이 적은 Presto로 데이터를 조회하는 구조
    - `Hive`: 분산 스토리지 내 비구조화 데이터 → 구조화 데이터 → 열 지향 스토리지 형식으로 저장
    - `Presto`: 완성한 구조화 데이터를 결합, 집계하고 비정규화 테이블로 데이터 마트에 내보낸다

<img src="https://user-images.githubusercontent.com/40620421/186680026-d52e7605-9ea2-47a9-87e0-d35e199999ca.png" width="500">

<br/>
<br/>

#### Hive에 의한 구조화 데이터 만들기

```sql
-- 예시 1) 바로 조회
SELECT status, count(*) cnt FROM access_log_csv
-- 8.664 seconds (바로 집계는 비효율적)

-- 예시 2) 구조화 진행 후 조회
CREATE TABLE access_log_orc STORED AS ORC AS
SELECT cast(TIME as timestamp) time, request, status FROM access_log_csv
-- 15.993 seconds (시간이 걸리는 프로세스임으로 Hive와 같은 배치형 쿼리 엔진이 적합)

SELECT status, count(*) cnt FROM access_log_csv
-- 1.567 seconds (구조화 데이터를 집계하는 것이 훨씬 효율적)
```

Hive로 비정규화 테이블 작성  
- 데이터 마트를 구축하기 위해 비정규화 테이블을 작성한다
- 시간이 오래 걸리는 작업인 경우 지연시간이 총 작업시간에 큰 영향을 주지 않고, 리소스 이용 효율을 높일 수 있어 Hive를 활용하는 것이 원칙적이다

Hive 쿼리 개선  
1. 서브 쿼리 안에서 레코드 수 줄이기

    ```sql 
    -- 비효율적인 쿼리 (전체 데이터를 조회한 후 Filter)
    SELECT ... 
    FROM access_log a 
    JOIN users b ON b.id=a.user_id
    WHERE b.created_at = '2017-01-01'

    -- 보다 효율적인 쿼리 (초기에 팩트 테이블을 작게한다)
    SELECT ... 
    FROM (
        SELECT * access_log
        WHERE time >= TIMESTAMP '2017-01-01 00:00:00'
    ) a
    JOIN users b ON b.id=a.user_id
    WHERE b.created_at = '2017-01-01'
    ```

1. 데이터 편향 피하기

    ```sql
    -- 비효율적인 쿼리 (distinct count는 분산되지 않아 처리가 오래 걸림)
    SELECT date, count(distinct user_id) users
    FROM access_log GROUP BY date

    -- 보다 효율적인 쿼리 (최초에 중복을 없앤다)
    SELECT date, count(*) users
    FROM(
        SELECT distinct date, user_id FROM access_log
    ) a
    GROUP BY date
    ```

#### 대화형 쿼리 엔진 Presto의 구조 
- `대화형 쿼리 엔진`: 쿼리 실행 지연을 감소하여 작은 쿼리를 여러 번 실행하는 엔진
- BigQuery Impala, Presto 등

`Presto`  
- 플러그인 가능한 스토리지
    - Hive와 같이 하나의 쿼리에서 `여러 개의 데이터 소스 연결 가능` (전용 스토리지 X)
        - Hive의 메타 스토어를 활용할 수 있음 → Hive와 연동성이 좋다
        - 열 지향 데이터 구조(ORC)로 되어있을 때 최대 성능 → 따라서 Hive에서 구조화된 데이터를 가져옴
    - MPP DB의 경우, 스토리지와 컴퓨팅 노드가 밀접한 연관
- CPU 처리의 최적화
    - 쿼리를 분석하여 최적의 실행 계획을 생성 → 워크 노드에 배포 → 병렬로 처리
    - CPU 이용 효율이 높음 (CPU와 메모리 성능이 좋으면 최대 성능)
    - 실행이 시작되면 중간에 끼어들 수 없음 → `너무 큰 쿼리 적합하지 않음`
- 인 메모리 처리에 의한 고속화
    - Hive와 달리 쿼리 실행 과정에서 디스크에 쓰지 않음
    - 메모리가 부족하면 여유가 생길 때까지 기다리거나 오류
    - 너무 오래 걸리는 작업은 Hive에 맡기는 게 좋음
- 분산 결합과 브로드캐스트 결합  
    테이블 결합 시 조인키를 메모리 상에 유지하는 방법
    - 분산 결합: 같은 키를 갖는 데이터는 동일한 노드에 모임 → 노드 간에 데이터 전송이 쿼리 지연을 초래함
    - 브로드캐스트 결합: 한쪽 테이블이 작을 경우, 결합하는 테이블에 모든 데이터가 각 노드에 복사된다  
    <img src="https://user-images.githubusercontent.com/40620421/187230686-67a9ad4b-9954-4844-a251-5e96ba958357.png" width="350">
    <img src="https://user-images.githubusercontent.com/40620421/187230707-140fc800-3324-47d4-8b77-45a937e3b1e2.png" width="350">

### 데이터 분석의 프레임워크 선택하기
- MPP DB
    - 구조화 데이터를 SQL로 집계하는 것뿐이라면 좋은 선택
    - 스토리지와 계산 노드가 일체화 (확장성, 유연성이 안 좋음)
    - BI도구와 조합에서 오랜 실적이 있어 시각화용 데이터 마트에 구축에 좋음
- Hive
    - 높은 확장성, 내결함성(안정성), 대규모 배치처리
    - 지연시간이 길어서 무거운 처리에 적합
        - 열 지향 스토리지 생성, 텍스트 데이터 가공
- Presto
    - 속도가 빠른 대화형 쿼리 엔진
    - 안정성이 떨어짐
        - 메모리가 부족하면 쿼리실행 불가
        - 실행 중 장애가 나면 처음부터 다시 실행
    - 표준 SQL 준수, 데이터 분석을 위해 자주 사용하는 쿼리엔진
    - 다양한 데이터 스토어에 대응 (Hadoop, MySQL, Cassandra, MongoDB)
- Spark
    - 인메모리 데이터 처리가 중심
    - ETL 프로세스에서 SQL에 이르기 까지의 흐름을 하나의 데이터 파이프라인으로 기술할 수 있음
    - SparkSQL을 통해 주력-SQL로 활용할 수도 있음

## 3-3. 데이터 마트 구축
시각화를 위해 데이터 마트를 구축  

### 팩트 테이블
- 작은 팩트 테이블을 모두 메모리에 올릴 수 있지만, 아니라면 열 지향 스토리지로 변환 필요
    - 이러한 구조화 과정에 대상 1순위
- 작성 방법
    1. 추가: 새로 도착한 데이터만을 증분으로 추가
    1. 치환: 과거의 데이터를 포함하여 테이블 전체를 치환

#### 테이블 파티셔닝
- `논리적인 하나의 테이블`을 `여러 물리적인 파티션`으로 나눔으로써 정리할 수 있음
- 추가하는 것이 효율이 더 좋지만, 단점 존재 → 이러한 가능성을 줄임
    1. 추가 실패에서 알아채지 못 하면 결손이 발생
    1. 추가를 잘못해서 여러번 실행하면 중복
    1. 다시 팩트 테이블을 만들고 싶을 때 관리가 복잡해짐
- 1일 1회, 1시간 1회라는 식으로 자주 새 파티션을 만들고 그것을 팩트 테이블에 붙여 놓음
- 많은 데이터를 다룰 때 용이 → `DW를 구축하는데 유용`
- 장점
    1. Select 쿼리 성능 향상
    1. 디스크 장애 시 파티션만 영향을 받음으로 데이터 훼손 가능성이 감소
    1. 조인시 파티션 간의 병렬 처리 및 파티션 내 병렬 처리 가능
    1. 파티션 단위로 디스크 I/O를 분산해 부하 감소
- 단점
    1. 파티션 키 값 변경에 대한 관리 필요
    1. Insert 속도가 느려짐
    1. Join에 대한 비용 증가
    
#### 데이터 마트의 치환
- 테이블을 모두 다시 쓴다. (일일 보고서를 위해 지난 30일 동안 데이터를 매일 꺼내 치환)
- 데이터 양이 아주 많지 않은 `데이터 마트에 사용됨`
- 장점
    1. 데이터가 중복되거나 빠드릴 가능성이 거의 없음
    1. 스키마 변경 등에 유연하게 대응
    1. 오래된 데이터는 자동으로 지워져 데이터 마트가 계속 확대되는 일이 없음
- 단점
    1. 처리 시간이 오래걸림 
        - 데이터가 많다면 문제가 커짐
        - 데이터 처리가 1시간 이내에 완료되지 않는다면 추가+파티셔닝 고려

### 집계 테이블
- 팩트 테이블을 어느 정도 모아 집계하여 데이터 양을 줄인다
- 칼럼이 취하는 값의 범위를 `카디널리티`라고 함
    - 집계 테이블을 구성할 때 카디널리티를 줄여야함
    - 다만 너무 낮으면 정보 손실이 되는 것을 주의

### 마스터 테이블
- 팩트 테이블에서 참고되는 각종 정보
    - 상황에 따라 일부 데이터가 업데이트됨
    - 고객 테이블에서 고객ID는 변경되지 않지만, 고객 주소와 같은 속성은 변경될 수 있음
- 이를 기록하기 위한 방법
    1. 스냅샷 테이블
        - 테이블을 통째로 저장하는 방법
        - 취급이 쉬움
        - 스냅샷은 기간의 시작이 아니라, 끝에 저장되야함
    1. 이력 테이블
        - 변경 내용만을 저장하는 방법
        - 데이터 양을 줄이는 데 도움이 됨
        - 나중에 복원하기가 어려워짐

### 비정규화 테이블 완성
- 팩트 테이블과 디멘전 테이블을 결합하여 비정규화 테이블을 만든다
    - 디멘전 테이블: 스냅샷 뿐만 아니라 목적에 따라 각종 중간 테이블이 만들어짐
- 카디널리티가 너무 크면 시각화가 제대로 되지 않기에 신경 써야함

```sql
SELECT
    date_trunc('day', a.time) time -- 1일 단위 그룹화 (디멘전)
    date_diff('day', b.min_time, a.time) days -- 방문한 후 걸린 일 수 (추가 디멘전)
    count(*) cnt
FROM(
    SELECT time, session_id FROM access_log -- 팩트 테이블에서 필요한 칼럼만 추출
    WHERE time BETWEEN TIMESTAMP '2017-01-01' AND TIMESTAMP '2018-01-01' -- 집계 기간
) a
JOIN sessions b ON b.session_id=a.session_id -- 디멘전 테이블과 결합
GROUP BY 1, 2 -- 1, 2열을 기준으로 그룹화
```

<br/>
<br/>
<br/>


# 4. 빅데이터의 축적

## 4-1. 벌크형과 스티리밍형의 데이터 수집
각 방법으로 분산 스토리지에 데이터가 저장될 때까지 흐름을 살펴봄

### 객체 스토리지
- 빅데이터에서 대부분의 경우 확장성이 높고, 대량으로 파일을 저장하기 위해 `분산 스토리지``객체 스토리지`를 사용
- 파일 읽고, 쓰기는 네트워크를 거쳐서 실행 → 데이터가 늘어나도 성능이 떨어지지 않음
- 데이터는 여러 디스크에 복사되어 내결함성을 가짐
- 통신 오버헤드가 커서 소량의 데이터에서는 비효율적
- HDFS, Amazon S3

#### 데이터 수집
- 수집한 데이터를 가공하여 집계 효율이 좋은 분산 스토리지를 만드는 프로세스
- 작은 데이터는 모아서 하나로 만들고, 큰 데이터는 복수로 나누는 것을 고려
    - 대량의 작은 파일을 생성하는 것은 성능 저하
    - 파일 크기가 너무 증가하면 네트워크 전송에 시간이 걸려 예상치 못한 오류 발생

### 벌크형의 데이터 전송
- DB, 파일서버, 웹 서버 등에서 각각의 방식으로 정리해 데이터 추출
- 전통적인 DW에서 사용 
- 신뢰성이 좋음 (실패할 경우 나중에 재실행하기 쉬움)
- 데이터 전송을 위해 `ETL 서버`를 설치
    - ETL 서버에서 데이터를 구조화하여 DW에 전송한다
    - 정기적인 실행(1시간 or 하루)을 하여 그동안 축적된 데이터를 하나로 모은다 (데이터의 크기가 적당하도록 주기를 설정하는 것이 중요)
- 워크 플로 관리 도구와 조합이 좋음


<img src="https://user-images.githubusercontent.com/40620421/187693026-3d0ab85a-aaf2-4e77-b268-ed654cef1f25.png" width="500">


### 스트리밍형의 데이터 전송
- 계속해서 전송되어 오는 작은 데이터를 취급
    - 바로 생성되어 어디에도 저장되지 않는 데이터는 그 자리에서 바로 전송됨
    - 웹 브라우저, 모바일 앱, 센서 기기 등 
- `메시지 배송`: 다수의 클라이언트에서 데이터를 전송받는 방식
    - 데이터 양에 비해 통신 오버헤드가 커서 높은 수준의 서버 필요
    - 작은 데이터 쓰기에 적합한 No SQL 활용
    - `메시지 큐`, `메시지 브로커` 등의 중계 시스템을 활용하여 일정한 간격으로 꺼내고 모아서 함께 분산 스토리지에 저장

<img src="https://user-images.githubusercontent.com/40620421/187695073-50818484-b452-4bfa-9804-bd7cac5890a6.png" width="500">

## 4-2. 메시지 배송의 트레이드 오프 (성능X신뢰성)
- 클라이언트 수가 많아지면 스트리밍 형의 메시지 배송의 `성능`과 `신뢰성`을 둘 다 만족하기가 어려워짐

### 메시지 브로커
- 스토리지의 성능 문제를 해결하는 중간층 설치
- 기존 문제점
    1. 외부에서 오는 메시지 양을 제어할 수 없음
    1. 쓰기 빈도가 계속해서 증가하여 오류가 날 수 있음
    1. 오류가 발생하면 클라이언트에서 재전송을 하려고 함 (악순환)
- 해결방법
    1. 좋은 성능을 가진 분산 스토리지
    1. 데이터를 중간에서 일시적으로 축척하는 메시지 브로커 설치
- 생산자와 소비자
    - `생산자`가 메시지 브로커에 데이터를 보냄
    - `소비자`가 메시지 브로커에서 데이터를 꺼냄
- `메시지 라우팅`
    - 데이터가 복수의 다른 소비자에서 읽어들일 수 있음 
    - 메시지가 복사되어 데이터를 여러 경로로 분기시킴
- Apache Kafka, AWS Kinesis

### 신뢰성 있는 메시지 배송
- 메시지의 중복이나 누락이 발생한 경우 3가지 중 하나를 보장하도록 설계
1. `at most once`
    - 메시지는 한 번만 전송됨 (메시지를 절대 다시 보내지 않음)
    - 전송에 실패되어 사라질 가능성이 있음 (결손 발생)
    - 오류를 감지하더라도 "메시지가 보내지지 않았다"를 증명하기 어려울 때
        - 수신측에서는 데이터를 받았으나, ack가 반환되는데 에러가 나타난 경우, 다시 보내면 데이터가 중복된다
1. `exactly once`
    - 네트워크상에 분단된 노드 사이에 `코디네이터`를 통해서 통신 내용을 보장받을 수 있을 때 활용
    - 장점: 코디네이터가 문제를 해결하여 메시지를 한번만 보낼 수 있음
    - 단점: 코디네이터에 의존하게 되고, 시간이 많이 소요
1. `at least once`
    - 메시지가 재전송되어도 그것을 없앨 수 있는 구조라면 활용
    - 시퀀스 번호를 통해 같은 번호가 중복해서 오면 파기한다. (중복 제거)
    - 대부분의 메시지 브로커에서 활용

#### 중복 데이터 핸들링 방법
1. 오프셋을 이용한 중복 제거
    - 전송해야할 데이터에 파일명과 시작위치(`오프셋`)을 덧붙인다
    - 데이터가 중복되더라도 같은 장소에 덮어써진다
    - 벌크형 데이터 전송과 같이 데이터 양이 고정된 경우 잘 작동
1. 고유 ID에 의한 중복 제거
    - 스티리밍형의 메시지 배송에서 자주 사용
    - ID가 폭발적으로 늘어난다 → 최근에 받은 ID만 기억한다 (중복은 대부분 일시적인 통신 오류 때문임으로)
1. 종단 간의 신뢰성
    - 중간 경로를 모두 `at least once`로 통일하고, 모든 메시지에 고유 ID를 포함하도록 한다
    - 경로의 말단에서 중복 제거를 실행
1. 고유 ID를 사용한 중복 제거의 방법
    - NoSQL: 특성상 데이터를 쓸 때 고유ID를 지정한다. ID가 겹치면 덮어쓴다.
    - SQL: 객체 스토리지에 우선 저장한 뒤 대규모 데이터 처리(Hive)를 통해 중복 제거.

#### 데이터 수집 파이프라인
- 클라이언트에서 발생한 데이터를 분산 스토리지에 적합한 형태로 저장하는 일련의 과정
- 필요에 따라 시스템을 조합
    - 쓰기 성능에 불안함이 없다면 메시지 브로커 생략 가능
    - 다소 중복이 허용된다면 중복 제거 생략 가능
- 빅데이터 시스템에서는 매우 높은 성능이 중요함으로 아주 작은 중복은 무시하는 경향이 있다
    - 신뢰성이 중시되는 경우에슨 스트리밍형보다는 벌크형으로 데이터를 보내는 것이 좋다

## 4-3. 시계열 데이터의 최적화
스트리밍 형 메시지 배송에서 `메시지가 도착할 때까지의 시간지연`이 문제다.  
- `이벤트 시간`: 클라이언트 상에서 메시지가 생성된 시간
- `프로세스 시간`: 서버가 처리하는 시간

### 프로세스 시간에 의한 분할
- 프로세스 시간과 이벤트 시간은 며칠까지도 지연이 된다
- 데이터 분석의 시간은 이벤트 시간임으로 문제가 발생
    - 1월 1일에 발생한 이벤트는 2월까지도 수집될 수 있다. 만약 프로세스 시간에 의한 분할이 되어있다면 `풀 스캔`을 해야함
    - 시간과 자원을 매우 낭비

#### 해결 방법
1. 시계열 인덱스
    - 이벤트 시간에 대해 인덱스를 만듬
    - 단기간 데이터 집계를 빠르게 실행 가능 (정해진 시간에 발생한 이벤트 조사, 실시간 대시보드)
    - 장기간 데이터의 경우 크게 효율적이지 않음 (열 지향 스토리지 활용 필요)
1. 조건절 푸쉬다운
    - 이벤트 시간으로 데이터를 정렬한 후에 `열 지향 스토리지`로 변환
        - 각 칼럼의 min/max 값이 메타 정보로 저장됨
    - 메타 정보를 활용하여 필요 최소한 데이터만을 읽도록 하는 방법
        - 예. 1월 1일 데이터를 집계하고 싶은데 이벤트 시간 스토리지의 최대값이 12월 31일이면 탐색할 필요가 없어진다
    - 너무 짧은 주기를 통해 만들면 많은 스토리지를 탐색해야함으로 적당한 크기로 만들어야함

### 이벤트 시간에 의한 분할
- `시계열 테이블`: 시간을 이용하여 분할된 테이블
    - 1월 1일에 발생한 이벤트라면 'event_0101'에 저장
- `데이터 마트`를 만드는 단계에서 이벤트 시간에 의한 정렬을 함께 하도록 하는 방안이 적절함
- 분산 스토리지에 대량의 작은 파일이 만들어지고, 쿼리 성능이 악화될 수 있음
    - 작은 데이터를 효율적으로 추가할 수 있는 분산 DB를 활용하거나 오래된 데이터를 버리는 아이디어 필요


## 4-4. 비구조화 데이터의 분산 스토리지
- NoSQL DB를 활용하여 데이터 수집. 실시간 집계를 할 수 있음

### NoSQL DB 활용
- 빅데이터를 위한 분산 스토리지 (`확장성`과 `유연성`을 갖춤)
- RDB의 한계를 뛰어넘기 위해서 개발됨. ACID특성을 부분적으로 포기하거나 제약을 마련함으로써 높은 성능을 실현
- 장점
    - 임의의 파일을 저장할 수 있음 
    - 대용량의 데이터를 저장
    - 고정되지 않은 스키마 
    - 읽기 성능이 좋음
    - 애플리케이션에서 오는 `데이터를 처음에 기록하는 장소`로 이용됨
- 단점
    - 파일 교체/변경이 어려움
    - `집계에 시간이 걸림` (열 지향 스토리지로 고속화할 수 있지만, 만드는 것도 시간이 걸림)
    - 데이터 일관성을 보장받지 않음
    - 중요한 데이터는 트랜잭션 처리를 고려한 DB에 기록
    - 성능 향상을 위한 색인 작성이 제한됨

### ACID와 CAP
- `ACID`: 일반적인 RDB가 이를 충족하여 신뢰성 있는 트랜잭션 처리
    1. 원자성(Atomicity): 모두 수행되었거나 수행되지 않았거나를 보장 (all or nothing)
    1. 일관성(consistency): DB의 모든 데이터는 여러 가지 조건, 규칙에 따라 유효함을 가져야 함
    1. 독립성(isolation): 트랜잭션 수행 시 서로 끼어들지 못하는 것
    1. 지속성(durability): 성공적으로 수행된 트랜잭션은 영원히 반영
- `CAP`: 분산 시스템은 ACID를 만족하기 어려움. 그 한계에 대해서 새롭게 만들어진 정리
    1. 일관성(consistency): 다중 클라이언트에서 같은 시간에 조회되는 데이터는 항상 동일함
    1. 가용성(availability): 모든 클라이언트의 읽기/쓰기 요청에 대하여 항상 응답 가능함. 몇 개의 클러스터가 장애가 너다라도 서비스 가능
    1. 네트워크 분할 허용성(patition-tolerance): 네트워크 데이터의 유실이 일어나도 각 지역의 시스템은 정상적으로 작동
- 결과 일관성
    - 가용성을 우선하고, 일관성을 포기
    - 시간이 지나면 언젠가 최신 데이터를 읽을 수 있음을 보장하지만, 그것이 언제일지는 알 수 없다

### 분산 Key-Value Store
- 모든 데이터를 `키-값 쌍`으로 저장 (가장 간단)
- 키를 통해서 노드 간에 부하를 균등하게 분산할 수 있음
- 마스터/슬레이브 구조로 마스터가 중지되면 데이터에 접근 가능

#### Amazon DynamoDB
- 안정된 읽기 쓰기 성능을 제공
- 하나 또는 두 개의 키를 연결
- 미리 설정한 초 단위의 요청 수에 따라 노드가 증감한다. → 데이터 읽고, 쓰기에 지연이 적음
- P2P 구조로 모든 노드가 대등한 관계고, 어떤 노드에 연결해도 데이터에 접근 가능

<img src="https://user-images.githubusercontent.com/40620421/
188469263-3c656ac0-a472-456f-9a60-15dab09cc219.png" width="500">

### 와이드 Column Store
- `하나의 키에 여러 개의 칼럼 이름과 칼럼 값의 쌍`으로 이루어진 데이터 (Key-Value 확장)
- 새로운 행을 추가하는 것처럼 열도 얼마든지 추가 가능
- 주로 `성능 향상`에 목적
    - 예. 1억 명이 사용하는 메시지 서비스의 경우, 사용자 ID를 키로 데이터를 분산하여 사용자별 타임라인을 구성할 수 있음
- 데이터 집계에 적합하지 않음 (분산된 모든 노드에서 데이터 모아야해서 시간이 오래 걸림) → Hive, Presto, Spark 등의 쿼리 엔진 활용
- Google Bigtable, Apache HBase, Apache Cassandra

<img src="https://user-images.githubusercontent.com/40620421/188477488-2bd47ef0-d8d2-4229-8132-327d9ab0a2d4.png" width="500">

#### Cassandra
- 오픈 소스의 와이드 칼럼 스토어
- 내부 데이터 저장소 기능. CQL을 통한 쿼리 언어로 구현
- 스키마를 결정할 필요가 있어 구조화 데이터만 취급 가능
    - RDB와 유사하지만, 쿼리의 의미가 다름 (INSERT INTO는 업데이트로 동작해 동일한 키를 가진 레코드를 덮어쓴다)
- P2P 분산 아키텍처. 지정한 키에 의한 노드에 관련된 모든 값이 저장됨
    - 노드 안에서 쿼리가 실행
    - 다수의 독립적인 키가 있는 경우 잘 분산됨

### 도큐먼트 스토어
- JSON처럼 복잡하게 뒤얽힌 스키마리스 데이터를 그대로 형태로 저장 및 쿼리 실행
- `데이터 처리의 유연성`을 목적으로 함
- 외부에서 들여온 데이터를 저장하는 데 특히 적합. (참고 시스템의 데이터, 로그 저장 등에 적합)

#### MongoDB
- 각종 프로그래밍 언어를 사용하여 데이터를 읽고 쓸 수있음
- 성능+간편함 >>> 신뢰성
- 데이터를 분산할 수 있지만 집계하는 데에 적합하지 않음. 쿼리 엔진으로부터 데이터 추출할 필요 있음

### 검색 엔진
- 텍스트 데이터 및 스키마리스 데이터를 집계하는 데 자주 사용
- 텍스트 데이터를 전문 검색하기 위해 `역 색인`을 만듬
    - **역 색인**: 텍스트에 포함된 단어를 분해하고, 어떤 단어가 어떤 레코드에 포함되어 있는가에 대한 인덱스를 만듬
    - 데이터를 쓸 때 부하가 커지지만, 키워드 검색이 고속화됨
    - 검색 엔진을 활용하지 않고, 전체 스캔을 하는 경우도 있음 (Google BigQuery)
- 실시간 집계 시스템 O, 장기적인 데이터 축적 X
    - 민첩성이 요구되는 용도에서 최근 데이터를 보기위해 활용   
    (실시간 집계 시스템, 비정상적인 상태의 감지 및 보안 체크, 고객 서포트)

#### Elasticsearch
- 모든 필드에 색인이 만들어짐 (텍스트 데이터에서는 역 색인)
    - 쓰기의 부하가 크다
    - 거의 실시간으로 저장, 검색, 분석 가능
- 샤드라고 하는 여러 컨데이터에 분산 저장, 장애 시 중복되는 사본을 제공 
- 임의의 JSON 데이터(document) 저장 가능
- 자체 쿼리 언어를 제공

<br/>
<br/>
<br/>

# 5. 빅데이터의 파이프라인

## 5-1. 워크플로 관리
- 정해진 업무를 원활하게 진행하기 위한 구조

### 워크플로 관리 도구
- 정기적으로 태스크를 실행하고, 비정상적인 상태를 감지하여 그것에 대한 해결을 돕는 것
    - 태스크 실행에 실패했을 때 재실행 등의 조치를 하기 위함
- 기존에는 업무용으로 개발된 관리 도구가 데이터 처리를 했지만, 최근에는 데이터 파이프라인에 특화된 도구가 개발됨
- 기본 기능
    1. 태스크를 정기적인 스케줄로 실행, 결과 통지
    1. 태스크 간에 의존 관계를 정하고, 정해진 순서대로 빠짐없이 실행하기
    1. 실행 결과를 보관하고, 오류 발생 시에 재실행할 수 있도록 하기
- 종류
    1. 선언형
        - XML이나 YAML 등의 서식으로 워크플로를 기술하는 타입
        - 미리 제공된 기능만을 사용가능, 유지 보수성이 높음
        - Oozie (하둡 호환성이 좋음)
    1. 스크립트형
        - 스크립트 언어로 워크플로를 정의하는 유형
        - 태스크 정의를 프로그래밍할 수 있어 `유연성`이 높음
        - 데이터를 변환/수집하는 복잡한 태스크를 구성할 수 있음
            - ETL 프로세스는 스크립트형의 도구, SQL 실행에는 선언형 도구로 활용하는 것도 방법
        - Airflow

#### 오류 복구 방법
데이터 파이프라에서는 예기치 못한 오류가 분명 발생   
→ 모든 오류를 사전에 예상하는 것이 불가능 함으로 `대처 방법`을 결정하는 것이 중요   

1. 복구와 플로우 재실행
    - 기본적으로 오류는 자동 회복할수 없다고 가정함 → 수작업에 의한 `복구`를 전제로한 태스크 설계
    - 동일 파라미터로 재실행했을 때 복구가 가능하도록 각 `플로우`를 설계
        - `플로우`: 워크플로 관리 도구에 의해서 실행되는 일련의 태스크
        - 일별 배치 처리라면 특정 날짜가 파라미터가 된다
        - 과거에 실패한 플로우와 파라미터를 DB에 기록하고, 그것을 재실행하는 것만으로도 복구가 되도록 설계
        - 이미 성공한 태스크는 넘어가고, 미완료 태스크만 실행하는 것도 가능
    - 태스크의 크기는 적당하게 하는 것이 좋음
        - 태스크가 너무 크면, 실행시 시간이 너무 오래 걸림 (재실행에 부담)
        - 태스크가 너무 작으면, 오버헤드가 커서 비효율적임
1. 재시도
    - 여러 번 발생하는 오류에 대해서 `자동화`하여 수작업없이 복구
        - 재시도 간격을 5분이나 10분 정도로 두면 성공할 수도 있음
        - 이상적으로는 재시도 없이, 오류의 원인을 계속 찾는 것이 좋음
    - `재시도 횟수`에 주의
        - 재시도가 적으면, 장애가 복구되기도 전에 재시도를 종료해 태스크가 실패
        - 재시도가 너무 많으면, 태스크가 실패되지 않은 것처럼 되어 중대한 문제를 발견하지 못할 수 있음
    - `데이터 중복`에 주의
        - 실패하더라도 데이터 전송에 성공한 경우도 있음
1. 백필(backfill)
    - 플로우 전체를 처음부터 다시 실행
    - 파라미터에 포함된 일시를 순서대로 바꿔가면서 일정 기간의 플로우를 연속해서 실행
    - 대량의 태스크를 실행할 때 성능상 주의가 필요
        - 많은 일자를 실행하면 평소 발생하지 않았던 에러도 나타날 수 있음
        - 자동 재시도는 모두 무효로 하고, 오류는 모두 통지하는 편이 좋음

### 멱등한 조작으로 태스크 작성
- `멱등`: 연산을 여러번 적용하더라도 결과가 달라지지 않는 성질
- 복구의 전제로써 기억해야할 것은 `재실행의 안정성`이다
    - 태스크 도중 실패했을 때, 그 경과가 사라지지 않고 남아있다면 데이터가 혼재할 수 있음
    - '마지막까지 성공' 또는 '실패하면 아무것도 남지 않음'

<br/>

1. 원자성 조작
    - 각 태스크가 시스템에 변경을 가하는 것을 한번만 실행
        - 여러번 쓰기를 한 번의 트랜잭션에 실행
        - 쓰기가 필요한 수만큼 태스크를 나누어 실행
    - 원자성 조작 직후에 문제가 발생하면, 데이터가 중복될 수 있음
        - 아주 작은 가능성도 허용하지 않을 때는 원자성 조작에 의존한 플로우 지양
1. 멱등한 조작 
    - 동일한 태스크를 여러번 실행해도 동일한 결과
        - 원칙적으로 항상 데이터를 덮어써야 함 
        - 태스크는 추가 또는 치환 중 하나를 실시 (추가는 중복이 되지만, 치환은 멱등함)
    - SQL의 경우 '테이블을 삭제한 후 다시 만들기'
    ```sql
    DROP TABLE IF EXISTS "TABLE1";  -- 테이블이 있다면 삭제
    CREATE TABLE "TABLE1" (...);    -- 테이블 생성
    INSERT INTO "TABLE1" ...;       -- 생성된 테이블에 데이터 삽입
    ```

<br/>

1. 멱등한 추가
    - 치환이 멱등성을 가지지만, 기존 데이터를 모두 치환할 경우 과부하가 걸릴 수 있다.
    - `테이블 파티셔닝`: 테이블을 파티션으로 분할하고, 파티션 단위로 치환
        - TRUNCATE나 INSERT OVERWRITE문 등의 효율 좋은 명령어를 사용할 수 있음
1. 원자성을 지닌 추가
    - 태스크를 멱등으로 구성하는 것이 어렵다면 고려해볼만한 방법
    - `중간 테이블`을 만들어 처리한 후, 마지막에 목적 테이블에 한번에 추가
    - 테이블을 치환함으로 멱등하면서도, 마지막은 단순 추가임으로 전체적으로는 멱등하지 않음

#### 워크플로 전체를 멱등으로 하기
- 재실행의 안정성을 높이기 위해서는 적어도 각 플로우가 전체로서 멱등하게 되도록 구현해야함
    - 특히 추가가 문제가 됨으로 이를 주의해야함
- 데이터 수집
    - 테이블 파티셔닝을 더입함으로써 파티션 단위 치환이 가능
    - 벌크형 데이터 전송에서도 날짜와 시간을 파라미터로 치환형 태스크 구현 가능
- 데이터 마트 구축  
    - 추가는 삼가하고, 테이블마다 치환하도록 한다
    - 중간 테이블도 치환하는 것이 바람직하지만, 성능상 추가할 수도 있음

### 태스크 큐
- 외부 시스템의 부하 컨트롤을 도움
- 태스크의 크기나 동시 실행 수를 변화시킴으로써 자원의 소비량을 조정한다
- 모든 태스크를 큐에 저장하고, 워커 프로세스가 순서대로 꺼내면서 병렬화를 실행
- 워커를 늘리면 실행 속도를 높일 수 있지만, 너무 증가시키면 어디선가 병목 현상이 발생하여 성능 향상이 한계점에 도달하거나 오류가 발생

## 5-2. 배치 형의 데이터 플로우
- `워크 플로우`: 데이터가 흐르는 전체 과정
- `데이터 플로우`: 데이터가 세세하게 조작되는 과정 
    - 분산 스토리지에 데이터가 전송되면 분산 시스템 프레임워크를 사용하여 
    - 문자 변환, 단어 개수 세기 등

### 데이터 플로우와 워크 플로우 조합
- 태스크를 정기적으로 실행하거나, 실패한 태스크를 기록하여 복구하는 것은 데이터 플로우에서 불가능  
    데이터 플로우를 워크 플로우의 일부로서 실행되는 하나의 태스크로 고려
- 데이터 읽기
    - 데이터 플로우에서 DB에 자주 접근하면 에러가 발생하기 쉽다
    - 워크 플로우를 통해서 DB에서 분산 스토리지로 데이터를 옮긴 후, 데이터 플로우를 진행한다
    <img src="https://user-images.githubusercontent.com/40620421/190162386-35b2de79-98a8-4e1b-a7ba-adab9e3f792e.png" width="500">
- 데이터 쓰기
    - 대량의 데이터 전송은 지양
    - CSV 파일과 같이 취급하는 쉬운 형식으로 변환하여 분산 스토리지에 쓴다 (데이터 플로우)
    - 벌크 형의 전송 도구를 사용하여 데이터를 전송 (워크 플로우)
    <img src="https://user-images.githubusercontent.com/40620421/190162418-1e7681df-f6b9-4223-9220-bb47aa4c0aab.png" width="500">

### 데이터 플로우와 SQL 나누어 사용
- 데이터 입출력에 더하여 SQL에 의한 쿼리 실행까지 조합 > 데이터 파이프라인 완성
- DW 구축
    - MPP DB에서 SQL을 실행하는 경우
    - 데이터 플로우: 비구조화 데이터를 가공하여 CSV 파일 등을 만들어 분산 스토리지에 전송
    - 워크 플로우: 태스크 실행이나 SQL에 의한 쿼리 실행
    <img src="https://user-images.githubusercontent.com/40620421/190162432-1088e44e-a3a0-4a53-b3e4-2c684a647803.png" width="500">
- 데이터 마트 구축
    - 분산 시스템 상의 쿼리 엔진을 실행하는 경우
    - 데이터 플로우: 구조화 데이터를 만듬 (분산 스토리지 상에 열 지향 스토리지 형식으로 보관)
    - 워크 플로우: 쿼리 엔진을 사용한 SQL 실행이나 그 결과를 데이터 마트에 전송
    <img src="https://user-images.githubusercontent.com/40620421/190162455-d2a523a2-5d8b-47f8-a31e-27232cc9f55f.png" width="500">

## 5-3. 스트리밍 형의 데이터 플로우
- `배치 처리`
    - 모아둔 데이터를 작게 처리하면 효율이 떨어지므로, 비교적 큰 단위로 데이터 처리
    - 장기적인 데이터 분석, 실시간 집계에는 적합하지 않음
- `스트림 처리`
    - 분산 스토리지를 거치지 않고 계속해서 처리 
    - 실시간 집계: 이벤트 발생 후 몇 초 후에 결과를 알 수 있도록 함
    - 과거 데이터를 재실시하는 것을 고려하지 않음
- Spark를 예로 데이터 플로우에서는 배치 처리와 스트림 처리를 같은 코드로 진행할 수 있다

### 스트림 처리 결과를 배치 처리로 치환하기
- 스트림 처리의 문제점
    1. 틀린 결과를 어떻게 수정할 것인가?
    1. 늦게 전송된 데이터는 어떻게 취급할 것인가?
- 이러한 문제점을 신뢰성이 높은 배치 처리로 치환하여 해결한다

#### 람다 아키텍처
- `배치 레이어`: 모든 데이터를 반드시 처리, 대규모 배치 처리를 실행하여 장기적인 스토리지에 축적한다.
- `서빙 레이어`: 배치 처리의 결과를 저장. 응답이 빠른 DB를 설치하여 집계 결과를 바로 추출. `배치 뷰` 제공
- `스피드 레이어`: 스트림 처리를 진행. `실시간 뷰` 제공 (배치 뷰가 업데이트될 동안까지만 제공)
- 배치뷰와 실시간 뷰를 모두 조합시키는 형태로 진행됨
<img src="https://user-images.githubusercontent.com/40620421/190168835-d613a33d-7aac-4eb5-b8df-6f6f19ff85f2.png" width="500">

#### 카파 아키텍처
- 람다 아키텍처에서 스피드 레이어와 배치 레이어가 같은 처리를 함으로 이를 보완
- 스피드 레이어만을 남기고, 데이터 보관 기간을 길게하여 문제가 일어났을 때 메시지 배송 시간을 과거로 설정
    - 과거의 데이터가 다시 스트림 처리되어 실질적인 재실행 진행
    - 스트림 처리가 멱등으로 되어있으면 출력 데이터가 덮어져 새로운 결과로 다시 쓰임
- 부하가 높아지는 문제가 발생할 수 있음

### 아웃 오버 오더 데이터 처리
- 스트림 처리의 또다른 문제는 늦게 도달하는 메시지
- 이벤트 시간 윈도윙
    - 시간을 일정 간격으로 나누어 `윈도우`를 만들고 그 안에서 데이터 집계
    - 한시간을 1분 간격의 60개의 윈도우로 나누는 것을 예로 들 수 있음