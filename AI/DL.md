DL

# Deep Learning Techniques

### Learning Rate
- LambdaLR: epoch에 따라서 지수함수를 따르면 lr이 줄어듬
- StepLR: Step마다 Learning Rate가 줄어듬
- CosineAnnealingLR: Cosine 함수를 따라서 주기성을 가지고 LR을 조정함
- ReduceLROnPlateau: patience epoch이 더 이상 개선되지 않을 때 learning rate를 감소

### Activations
- 인공신경망에서 뉴런의 출력을 결정하는 함수로, 입력 신호의 총 합을 계산하여 어떤 출력을 생성할지를 결정하는 역할
    - 비선형 함수이어야 하며, 이는 신경망이 복잡한 데이터 학습을 도움
    - 비선형 함수를 사용하지 않으면 인공신경망은 하나의 선형 변환만 수행할 수 있으므로 심층 네트워크의 이점을 활용할 수 없음
- 종류
    - Sigmoid 함수
        - 입력값을 0과 1 사이의 값으로 압축하여 활성화
        -  이진 분류 문제의 출력 뉴런에서 사용
        - f(x) = 1 / (1 + exp(-x))
    - Tanh 함수
        - Tanh 함수는 Sigmoid 함수와 유사하지만, -1과 1 사이의 값으로 압축
        - 중심을 기준으로 대칭적입니다.
        - f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
    - ReLU 함수
        - ReLU 함수는 입력값이 양수인 경우 그대로 출력하고, 음수인 경우 0을 출력하는 함수
        - 매우 간단하고 계산 비용이 저렴하기 때문에 주로 사용
        - f(x) = max(0, x)
    - Softmax 함수
        - Softmax 함수는 입력값들을 확률 분포로 변환
        - 출력 뉴런 중 하나가 가장 큰 값을 가지고, 나머지 출력 뉴런들은 상대적으로 작은 값들을 가짐
        - 주로 다중 클래스 분류 문제의 출력 뉴런에서 사용
        - f(x_i) = exp(x_i) / sum(exp(x_j)) for j = 1 to n (n은 출력 뉴런의 개수)

### Dropout
- 과적합(Overfitting)을 방지하기 위해 사용되는 정규화(regularization) 기법 중 하나
    - 신경망의 뉴런을 임의로 선택하여 학습 중에 제외시키는 방식으로 동작
    - 각각의 뉴런에 지나치게 의존하지 않고 더 일반화된 특징을 학습하도록 함
    - 일반적으로 훈련 단계에서만 적용되며, 테스트 단계에서는 비활성화
    - 0.2에서 0.5 사이의 드롭아웃 비율이 사용
- 완전 연결 뉴런 계층(Fully Connected Layers)에서 많이 사용되지만, 합성곱 신경망(CNN) 등 다른 신경망 구조에서도 적용 가능
- 학습 시 노드가 무작위로 비활성화되기 때문에, 학습 과정에서는 일반적으로 더 많은 에폭(epoch)이 필요

### Batch Normalization
- DL 학습 과정을 안정화하고 속도를 높이며, 과적합을 줄이는 정규화(regularization) 기법 중 하나
    - 입력 데이터의 분포를 정규화하여, 각 층에서 활성화 함수를 통과하기 전에 데이터의 분포를 안정화시키는 방식으로 동작
    - 각 미니 배치(mini-batch)의 평균과 분산을 계산하여, 이를 이용해 정규화된 데이터를 얻음
- 과정
    1. 미니 배치의 평균과 분산 계산
    1. 정규화: 미니 배치의 평균과 분산을 이용하여 입력 데이터를 정규화. 이를 통해 데이터의 분포를 평균이 0이고 분산이 1인 형태로 조정.
    1. 스케일과 시프트: 정규화된 데이터에 대해 스케일(scale)과 시프트(shift)를 수행. 스케일과 시프트는 학습 가능한 파라미터로서, 각 층의 입력에 적합한 크기와 평균값을 가질 수 있도록 함
- 장점
    - 학습 안정화: 학습 과정에서 그래디언트 소실 또는 그래디언트 폭주 문제를 완화하고, 학습 과정을 안정화
    - 학습 속도 향상: 배치 정규화를 사용하면 학습률(learning rate)을 크게 설정할 수 있어, 학습 속도가 향상
    - 더 깊은 네트워크 구조: 배치 정규화를 통해 깊은 신경망을 사용할 수 있으며, 그로 인해 더 복잡한 문제를 해결하는데 도움

### Layer Normalization
- DL 정규화(regularization) 기법 중 하나
    - 각 층(layer)을 기준으로 정규화 (배치 정규화(Batch Normalization)과 유사한 개념)
- 특징
    - 미니 배치 크기에 의존하지 않음: 레이어 정규화는 미니 배치 크기에 영향을 받지 않기 때문에 작은 미니 배치 크기로도 잘 동작
    - 학습 속도: 레이어 정규화는 학습 속도가 빨라집니다. 배치 정규화와 달리 학습 단계에서 평균과 분산을 구하는 연산이 필요 없기 때문에 더 빠른 학습이 가능
    - 특정 시퀀스 데이터에 적합: 레이어 정규화는 각 층의 입력을 정규화하기 때문에, 시퀀스 데이터 처리에 적합

### Residual Connection
- 네트워크의 일부를 건너뛰어 보다 깊은 네트워크를 구성하는 데에 사용
    - 네트워크가 더 깊어질 때 발생할 수 있는 그래디언트 소실(Gradient Vanishing) 문제를 완화
        - `Gradient Vanishing`: 입력이 매우 크거나 매우 작을 때, 활성화 함수의 기울기가 거의 0에 가깝게 되는 특성
    - 더 쉽게 최적화하고 학습
- 동작 
    - 입력과 출력 사이의 연결
        - 입력과 출력 사이를 바로 연결하는 방식으로 동작
        - 네트워크의 일부를 건너뛰어 출력에 입력을 더해주는 방식
    - 잔차(Residual)
        - 입력과 출력의 차이인 잔차(Residual)를 학습하도록 모델이 구성
        - 네트워크가 잔차를 학습하여 더하는 것을 통해, 네트워크가 더 쉽게 최적화될 수 있으며, 기존의 네트워크보다 더 깊은 구조를 학습
- 수식
    - y = F(x) + x
    - x: 입력 데이터, F(x)는 x를 입력으로 받아 네트워크를 거친 후의 출력



# Singular Value Decomposition
- 행렬을 세 개의 행렬의 곱으로 분해하는 기법
    - 잠재적인 패턴과 중요한 정보를 추출하는 데에 널리 사용되며, 데이터 압축, 차원 축소, 행렬 근사 등 다양한 분야에서 유용하게 적용
    - 특이값들 중 상위 몇 개만 남기고 나머지는 0으로 만들어서 데이터의 차원을 축소하거나 노이즈를 제거하는 등의 목적으로 사용
- A = U * Σ * V^T
    - A는 m x n 크기의 원본 행렬
    - U는 m x m 크기의 직교 행렬(orthogonal matrix)로, A의 왼쪽 특이벡터를 열로 갖음
    - Σ는 m x n 크기의 대각 행렬(diagonal matrix)로, 특이값(singular value)들이 내림차순으로 정렬되어 있음. 대각 성분 이외의 값은 모두 0.
        - Singular Value가 클수록 원본 매트릭스를 구성하는데 큰 기여를 함
    - V^T는 n x n 크기의 직교 행렬로, A의 오른쪽 특이벡터를 열로 갖음
- Face Image 예시
    - U: m x m 행렬. A의 왼쪽 특이벡터를 열로 가짐.
    - Σ: m x n 행렬. 대각 성분에는 얼굴 이미지 데이터셋의 특이값들이 내림차순으로 정렬.
    - V^T: n x n 행렬. A의 오른쪽 특이벡터를 열로 갖음.
    - Rank를 적게 사용했는데도 Image에 가까워지면 이미지를 효율적으로 압축했다고 볼 수 있음

# Transformer
- 주로 자연어 처리(Natural Language Processing, NLP) 작업에 사용되는 모델
    - 기계 번역과 같은 시퀀스-투-시퀀스(Sequence-to-Sequence, Seq2Seq) 작업에서 성공적으로 사용
    - RNN 기반 모델보다 훨씬 뛰어난 성능
- 특징
    - 어텐션 메커니즘(Attention Mechanism)
        - 시퀀스 데이터의 길이와 관계없이 입력과 출력 간의 상호작용을 모델링
        - 입력 시퀀스의 모든 위치에서 다른 위치로 정보를 집중(Attention)하는 방식으로 작동
        - 각각의 단어가 다른 단어들과 상호작용하여 중요한 정보를 추출하는 능력을 가지고 있음
    - 인코더-디코더 구조
        - 인코더는 입력 시퀀스를 인코딩하여 중간 표현을 생성
        - 디코더는 인코더의 출력과 이전의 출력을 이용하여 출력 시퀀스를 생성
        - 인코더와 디코더 사이의 어텐션 레이어를 통해 입력과 출력 간의 상호의존성을 모델링
        - Encoder의 마지막 output은 Decoder 모든 layer에 input으로 활용
    - 셀프 어텐션(Self-Attention)
        - 입력 시퀀스의 모든 위치에서 입력 시퀀스의 다른 위치로 정보를 전달하면서 입력 시퀀스 내의 각 워드의 관계를 모델링
        - 현재 위치의 앞의 단어를 확인
    - 포지션 임베딩(Positional Embedding)
        - 입력 시퀀스의 단어들이 순서를 가지고 있는 시퀀스 데이터이므로, 단어의 순서 정보를 표현하기 위해 포지션 임베딩을 사용
        - 모델은 단어의 상대적인 위치 정보를 학습

```
 +--------------------+
 |  Input Sequence   |         +-------------+
 +---------|----------+         |    Token    |
           |                    |   Embedding |
           V                    +-----|-------+
 +-------------------+                |
 |    Positional     |                V
 |     Encoding      |         +-------------+
 +---------|----------+         |   Add &    |
           |                    | Normalization|
           V                    +-----|-------+
 +-------------------+                |
 |     Encoder       |                V
 |   (Multiple       |         +-------------+
 |   Layers)         |         | Self-Attention|
 +---------|----------+         +------|------+
           |                           |
           V                           V
 +-------------------+         +-------------+
 |  Encoder Output   |         |  Feed-Forward|
 +---------|----------+         |   Network   |
           |                    +------|------+
           V                           |
 +-------------------+                V
 |     Decoder       |         +-------------+
 |   (Multiple       |         | Add & Normal|
 |   Layers)         |         |   & Attention|
 +---------|----------+         +-------------+
           |                           |
           V                           V
 +-------------------+         +-------------+
 |     Decoder      o|         | Feed-Forward|
 |    Output         |         |   Network   |
 +---------|----------+         +-------------+
           |
           V
 +-------------------+
 |    Output Layer   |
 +-------------------+
```


### positional encoding
- 입력 시퀀스의 단어들에게 위치 정보를 부여하는 기술
    - 각 단어 벡터에 위치 정보를 더하는 방식
- 수식 (pos: 단어위 위치, d_model: 임베딩 벡터의 차원 수)
    - PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    - PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

#### 예시
I = [0.2, 0.1, 0.5, 0.4]
love = [0.9, 0.6, 0.3, 0.7]
artificial = [0.3, 0.2, 0.8, 0.5]
intelligence = [0.7, 0.6, 0.1, 0.9]

각 단어 벡터에 Positional Encoding

PE(1, 0) = sin(1 / 10000^(0/4)) ≈ sin(1) ≈ 0.84
PE(1, 1) = cos(1 / 10000^(1/4)) ≈ cos(1) ≈ 0.54

PE(2, 0) = sin(2 / 10000^(0/4)) ≈ sin(2) ≈ 0.91
PE(2, 1) = cos(2 / 10000^(1/4)) ≈ cos(2) ≈ -0.42

PE(3, 0) = sin(3 / 10000^(0/4)) ≈ sin(3) ≈ 0.14
PE(3, 1) = cos(3 / 10000^(1/4)) ≈ cos(3) ≈ -0.99

PE(4, 0) = sin(4 / 10000^(0/4)) ≈ sin(4) ≈ -0.76
PE(4, 1) = cos(4 / 10000^(1/4)) ≈ cos(4) ≈ -0.65

이제 각 단어 벡터에 해당하는 위치의 Positional Encoding을 더함

새로운 I = [0.2, 0.1, 0.5, 0.4] + [0.84, 0.54, 0.91, -0.42] ≈ [1.04, 0.64, 1.41, -0.02]
새로운 love = [0.9, 0.6, 0.3, 0.7] + [0.91, -0.42, 0.14, -0.99] ≈ [1.81, 0.18, 0.44, -0.29]
새로운 artificial = [0.3, 0.2, 0.8, 0.5] + [-0.76, -0.65, 1.15, -0.72] ≈ [-0.46, -0.45, 1.95, -0.22]
새로운 intelligence = [0.7, 0.6, 0.1, 0.9] + [-0.76, -0.65, 1.15, -0.72] ≈ [-0.06, -0.05, 1.25, 0.18]


### Self-Attention 예시
1. 입력 시퀀스 인코딩
I = [0.2, 0.1, 0.5, 0.4]
love = [0.9, 0.6, 0.3, 0.7]
artificial = [0.3, 0.2, 0.8, 0.5]
intelligence = [0.7, 0.6, 0.1, 0.9]

1. 쿼리(Query), 키(Key), 값(Value) 계산:
쿼리(Q) = [0.2, 0.1, 0.5, 0.4]
키(K) = [0.2, 0.1, 0.5, 0.4]
값(V) = [0.2, 0.1, 0.5, 0.4]

1. 어텐션 가중치 계산:
어텐션 가중치(Attention Weights) = Softmax(Q * K^T)
Q * K^T = [0.2, 0.1, 0.5, 0.4] * [0.2, 0.1, 0.5, 0.4]^T = 0.26
softmax(0.26) = 0.58

1. 가중합 계산:
어텐션 가중치를 값(V)와 가중합하여 새로운 표현을 얻습니다.
새로운 I = 0.58 * [0.2, 0.1, 0.5, 0.4] = [0.116, 0.058, 0.29, 0.232]







