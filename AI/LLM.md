LLM

# LLM (Large Language Model)
- 텍스트 생성, 자연어 이해, 기계 번역 등 다양한 자연어 처리(Natural Language Processing, NLP) 작업을 수행
- 방대한 양의 텍스트 데이터를 기반으로 사전 훈련된 후, 특정 작업에 맞게 추가적인 훈련을 거침

## 자연어 처리(Natural Language Processing, NLP)
- 인간의 언어를 기계가 이해하고 처리할 수 있도록 하는 분야
    - 텍스트 데이터를 이해, 분석, 생성 및 해석하는 기술
    - 인간과 기계 간의 언어 커뮤니케이션을 가능하게 함
- 주요 작업
    1. 텍스트 분류
        - 주어진 텍스트를 사전 정의된 카테고리로 분류하는 작업 
        - 예. 스팸 메일 필터링, 감성 분석, 문서 분류 등
    1. 개체명 인식
        - 텍스트에서 중요한 개체(인물, 장소, 날짜 등)를 인식하고 분류하는 작업
        - 예. "Apple은 캘리포니아에 위치한 기업입니다"라는 문장에서 "Apple"을 조직명으로 인식
    1. 기계 번역
        - 한 언어로 작성된 문장을 다른 언어로 자동으로 번역하는 작업
        - 예. 구글 번역과 같은 온라인 번역 도구가 이 작업에 사용
    1. 자동 요약
        - 중요한 내용이나 주제를 추출하여 요약을 생성
    1. 질의 응답
        - 질문에 대한 정확한 답변을 생성하는 작업
- 대표 모델
    1. BERT 
        - 구글에서 개발한 사전 훈련된 언어 모델로, 양방향 Transformer 인코더를 기반
        - 문맥을 이해하는 데 중점을 두고 있으며, 문장의 좌우 문맥을 함께 고려하여 단어의 의미를 파악
        - 텍스트 분류, 개체명 인식, 질의 응답 등 다양한 NLP 작업에 사용
    1. GPT
        - OpenAI에서 개발된 사전 훈련된 언어 모델로, 텍스트 생성에 특화
        - 훈련 데이터에 기반하여 다음 단어를 예측하거나, 주어진 문장을 이어서 완성하는 등의 작업을 수행
    1. Transformer
        - 인코더와 디코더라는 두 개의 주요 구성 요소로 구성
        - self-attention(자기 어텐션) 메커니즘을 사용하여 문장의 단어 간 상호 작용을 모델링하며, 기계 번역과 같은 다양한 NLP 작업에 사용
        - BERT와 GPT는 Transformer 아키텍처를 기반




## Pre-traininng & Fine-tuning

### Pre-training
- 모델을 초기화하기 위해 대규모 텍스트 데이터로 모델을 훈련시키는 과정
    - 많은 양의 텍스트 데이터(인터넷 문서, 책, 뉴스 등)를 사용하여 모델은 문장 구조, 어휘, 문맥 등과 같은 언어의 다양한 특징을 파악하고 내재된 표현을 학습
    - 사전 훈련된 모델은 후속 작업을 위해 미세 조정(Fine-tuning) 단계로 이어질 수 있음
- 목적
    - 언어 이해의 일반화
        - 모델이 언어의 다양한 패턴, 구조, 문법, 단어 간 관계 등을 이해하는 일반화된 표현을 학습
        - 모델은 다양한 언어 작업에 적용될 수 있는 범용적인 언어 이해 능력을 갖출 수 있음
    - 데이터 희소성 극복
        - 많은 문장과 단어 조합이 가능한데, 각각의 조합에 대한 레이블이 부족하거나 없을 수 있음
    - 전이 학습
        - 사전 훈련된 모델은 일반 언어 이해 능력을 보유하고 있기 때문에, 특정 작업에 대한 미세 조정을 통해 해당 작업에 특화된 표현과 패턴을 학습
        - 데이터 양이 적은 작업이더라도 효과적인 학습

### Fine-tuning
- 사전 훈련된 모델을 특정 작업에 맞게 추가로 훈련시키는 과정
- 목적
    - 작업에 맞는 특화된 표현 학습
        - 특정 작업에 대한 레이블이 있는 작은 규모의 작업 관련 데이터를 사용하여 모델을 추가로 훈련
    - 작은 데이터셋에서의 효과적인 학습
        - 작은 데이터셋이나 특정 도메인에 특화된 작업에서도 효과적인 학습을 가능
        일반적인 언어 이해 능력을 기반으로 작은 데이터셋에서도 성능을 높일 수 있음
    - Overfitting 방지와 일반화 능력 향상
        - 특정 작업에 초점을 맞추기 때문에, 모델이 작업 관련 데이터에 과적합되지 않도록 제어


#### 예시
BERT를 기반으로 한 감성 분석 작업

1. 사전 훈련
    - BERT 모델은 대규모 텍스트 데이터를 사용하여 사전 훈련됨
    - BERT는 언어의 일반적인 특성을 학습하고 문장의 의미와 문맥을 이해하는 능력을 개발
1. Fine-tuning 데이터 수집
    - 감성 분석을 위한 작업 관련 데이터를 수집
    - 문장 또는 문서와 그에 대응하는 감성(긍정 또는 부정) 레이블로 구성
1. Fine-tuning
    - BERT 모델의 일부 레이어 또는 분류기를 수정하여 작업에 특화된 표현을 학습
    - 예를 들어, 출력 레이어를 감성 분류를 위한 이진 분류기로 교체하고, 해당 분류기를 훈련 데이터에 맞게 조정
1. 성능 평가
    - 테스트 데이터를 사용하여 모델의 성능을 평가하고 정확도, 정밀도, 재현율 등의 평가 지표를 계산

