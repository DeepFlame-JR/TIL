LLM

# LLM (Large Language Model)
- 텍스트 생성, 자연어 이해, 기계 번역 등 다양한 자연어 처리(Natural Language Processing, NLP) 작업을 수행
- 방대한 양의 텍스트 데이터를 기반으로 사전 훈련된 후, 특정 작업에 맞게 추가적인 훈련을 거침


## 자연어 처리(Natural Language Processing, NLP)
- 인간의 언어를 기계가 이해하고 처리할 수 있도록 하는 분야
    - 텍스트 데이터를 이해, 분석, 생성 및 해석하는 기술
    - 인간과 기계 간의 언어 커뮤니케이션을 가능하게 함
- 주요 작업
    1. 텍스트 분류
        - 주어진 텍스트를 사전 정의된 카테고리로 분류하는 작업 
        - 예. 스팸 메일 필터링, 감성 분석, 문서 분류 등
    1. 개체명 인식
        - 텍스트에서 중요한 개체(인물, 장소, 날짜 등)를 인식하고 분류하는 작업
        - 예. "Apple은 캘리포니아에 위치한 기업입니다"라는 문장에서 "Apple"을 조직명으로 인식
    1. 기계 번역
        - 한 언어로 작성된 문장을 다른 언어로 자동으로 번역하는 작업
        - 예. 구글 번역과 같은 온라인 번역 도구가 이 작업에 사용
    1. 자동 요약
        - 중요한 내용이나 주제를 추출하여 요약을 생성
    1. 질의 응답
        - 질문에 대한 정확한 답변을 생성하는 작업
- 대표 모델
    1. BERT 
        - 구글에서 개발한 사전 훈련된 언어 모델로, 양방향 Transformer 인코더를 기반
        - 문맥을 이해하는 데 중점을 두고 있으며, 문장의 좌우 문맥을 함께 고려하여 단어의 의미를 파악
        - 텍스트 분류, 개체명 인식, 질의 응답 등 다양한 NLP 작업에 사용
    1. GPT
        - OpenAI에서 개발된 사전 훈련된 언어 모델로, 텍스트 생성에 특화
        - 훈련 데이터에 기반하여 다음 단어를 예측하거나, 주어진 문장을 이어서 완성하는 등의 작업을 수행
    1. Transformer
        - 인코더와 디코더라는 두 개의 주요 구성 요소로 구성
        - self-attention(자기 어텐션) 메커니즘을 사용하여 문장의 단어 간 상호 작용을 모델링하며, 기계 번역과 같은 다양한 NLP 작업에 사용
        - BERT와 GPT는 Transformer 아키텍처를 기반

#### Scaling Laws for NLM (Neural Language Models)
- 모델의 크기나 매개변수 수가 증가함에 따라 모델의 성능이 어떻게 변하는지를 설명하는 법칙
- LM의 성능은 아래에 의존함
    - 모델 파라미터 수 (N)
    - 데이터셋의 사이즈 (D)
    - 컴퓨터링량 (C)


### Transformer
- 주로 자연어 처리(Natural Language Processing, NLP) 작업에 사용되는 모델
    - 기계 번역과 같은 시퀀스-투-시퀀스(Sequence-to-Sequence, Seq2Seq) 작업에서 성공적으로 사용
    - RNN 기반 모델보다 훨씬 뛰어난 성능
- 특징
    - 어텐션 메커니즘(Attention Mechanism)
        - 시퀀스 데이터의 길이와 관계없이 입력과 출력 간의 상호작용을 모델링
        - 입력 시퀀스의 모든 위치에서 다른 위치로 정보를 집중(Attention)하는 방식으로 작동
        - 각각의 단어가 다른 단어들과 상호작용하여 중요한 정보를 추출하는 능력을 가지고 있음
    - 인코더-디코더 구조
        - 인코더는 입력 시퀀스를 인코딩하여 중간 표현을 생성
        - 디코더는 인코더의 출력과 이전의 출력을 이용하여 출력 시퀀스를 생성
        - 인코더와 디코더 사이의 어텐션 레이어를 통해 입력과 출력 간의 상호의존성을 모델링
        - Encoder의 마지막 output은 Decoder 모든 layer에 input으로 활용
    - 셀프 어텐션(Self-Attention)
        - 입력 시퀀스의 모든 위치에서 입력 시퀀스의 다른 위치로 정보를 전달하면서 입력 시퀀스 내의 각 워드의 관계를 모델링
        - 현재 위치의 앞의 단어를 확인
    - 포지션 임베딩(Positional Embedding)
        - 입력 시퀀스의 단어들이 순서를 가지고 있는 시퀀스 데이터이므로, 단어의 순서 정보를 표현하기 위해 포지션 임베딩을 사용
        - 모델은 단어의 상대적인 위치 정보를 학습

```
 +--------------------+
 |  Input Sequence   |         +-------------+
 +---------|----------+         |    Token    |
           |                    |   Embedding |
           V                    +-----|-------+
 +-------------------+                |
 |    Positional     |                V
 |     Encoding      |         +-------------+
 +---------|----------+         |   Add &    |
           |                    | Normalization|
           V                    +-----|-------+
 +-------------------+                |
 |     Encoder       |                V
 |   (Multiple       |         +-------------+
 |   Layers)         |         | Self-Attention|
 +---------|----------+         +------|------+
           |                           |
           V                           V
 +-------------------+         +-------------+
 |  Encoder Output   |         |  Feed-Forward|
 +---------|----------+         |   Network   |
           |                    +------|------+
           V                           |
 +-------------------+                V
 |     Decoder       |         +-------------+
 |   (Multiple       |         | Add & Normal|
 |   Layers)         |         |   & Attention|
 +---------|----------+         +-------------+
           |                           |
           V                           V
 +-------------------+         +-------------+
 |     Decoder      o|         | Feed-Forward|
 |    Output         |         |   Network   |
 +---------|----------+         +-------------+
           |
           V
 +-------------------+
 |    Output Layer   |
 +-------------------+
```


### Masked language model
- 언어 모델링의 한 유형으로, 주어진 문장에서 일부 단어를 마스크하고, 마스크된 단어를 예측하는 모델
- 동작 방식
    - 입력 문장에 마스크
    - 양방향 학습 (문맥을 더 잘 이해하고 더 정확한 예측)
    - Fine-tuning (특정한 자연어 처리 작업(예: 문장 분류, 질의응답, 기계 번역 등)을 위함)

### 사전훈련을 위한 목적함수
- Next Sentence Prediction
- Shuffle Token Detection / Random Token Substitution
- Translation Language Model (다른 두 언어가 한쌍으로 토큰이 마스킹되어 사용)
- Span Boundary Objective (문장에서 연속된 토큰을 마스킹하여 토큰 예측)
- Sentence Order Prediction (연속된 두 문장을 추출하여 순서를 바꾸고, 이를 맞추는 지 확인)

### tokenization
- 텍스트를 작은 단위인 토큰(Token)으로 나누는 과정
    - 텍스트를 처리하고 분석하는 과정이 훨씬 용이해지며, 모델이 텍스트를 이해하고 처리하는데에도 도움
    - 텍스트 데이터를 숫자로 표현해야 하는 자연어 처리 모델에서 중요한 과정
    - 단어 간의 상관관계를 파악하고 텍스트의 의미를 추론하는 작업을 수행
    - 오타가 있을 때도 sub-word를 살릴 수 있음 (postfix, prefix도 검출 가능)
- 예시
    - "Natural language processing is interesting!"
        - Natural
        - language
        - processing
        - is
        - interesting
        - !


## Training

### Pre-training
- 모델을 초기화하기 위해 대규모 텍스트 데이터로 모델을 훈련시키는 과정
    - 많은 양의 텍스트 데이터(인터넷 문서, 책, 뉴스 등)를 사용하여 모델은 문장 구조, 어휘, 문맥 등과 같은 언어의 다양한 특징을 파악하고 내재된 표현을 학습
    - 사전 훈련된 모델은 후속 작업을 위해 미세 조정(Fine-tuning) 단계로 이어질 수 있음
- 목적
    - 언어 이해의 일반화
        - 모델이 언어의 다양한 패턴, 구조, 문법, 단어 간 관계 등을 이해하는 일반화된 표현을 학습
        - 모델은 다양한 언어 작업에 적용될 수 있는 범용적인 언어 이해 능력을 갖출 수 있음
    - 데이터 희소성 극복
        - 많은 문장과 단어 조합이 가능한데, 각각의 조합에 대한 레이블이 부족하거나 없을 수 있음
    - 전이 학습
        - 사전 훈련된 모델은 일반 언어 이해 능력을 보유하고 있기 때문에, 특정 작업에 대한 미세 조정을 통해 해당 작업에 특화된 표현과 패턴을 학습
        - 데이터 양이 적은 작업이더라도 효과적인 학습

### Fine-tuning
- 사전 훈련된 모델을 특정 작업에 맞게 추가로 훈련시키는 과정
- 목적
    - 작업에 맞는 특화된 표현 학습
        - 특정 작업에 대한 레이블이 있는 작은 규모의 작업 관련 데이터를 사용하여 모델을 추가로 훈련
    - 작은 데이터셋에서의 효과적인 학습
        - 작은 데이터셋이나 특정 도메인에 특화된 작업에서도 효과적인 학습을 가능
        일반적인 언어 이해 능력을 기반으로 작은 데이터셋에서도 성능을 높일 수 있음
    - Overfitting 방지와 일반화 능력 향상
        - 특정 작업에 초점을 맞추기 때문에, 모델이 작업 관련 데이터에 과적합되지 않도록 제어


#### 예시
BERT를 기반으로 한 감성 분석 작업

1. 사전 훈련
    - BERT 모델은 대규모 텍스트 데이터를 사용하여 사전 훈련됨
    - BERT는 언어의 일반적인 특성을 학습하고 문장의 의미와 문맥을 이해하는 능력을 개발
1. Fine-tuning 데이터 수집
    - 감성 분석을 위한 작업 관련 데이터를 수집
    - 문장 또는 문서와 그에 대응하는 감성(긍정 또는 부정) 레이블로 구성
1. Fine-tuning
    - BERT 모델의 일부 레이어 또는 분류기를 수정하여 작업에 특화된 표현을 학습
    - 예를 들어, 출력 레이어를 감성 분류를 위한 이진 분류기로 교체하고, 해당 분류기를 훈련 데이터에 맞게 조정
1. 성능 평가
    - 테스트 데이터를 사용하여 모델의 성능을 평가하고 정확도, 정밀도, 재현율 등의 평가 지표를 계산


### Parameter Efficient Fine Tuning (PEFT) 
- 대부분의 파라미터를 프리징하고 일부의 파라미터만 파인튜닝
    - 사전 학습된 LLM에 데이터셋을 활용해서 파인튜닝하는 것으로 성능향상을 할 수 있음
    - 기존의 사전 학습된 언어 모델을 추가 학습 없이, 주어진 평가 데이터에 적용하여 평가 점수를 얻을 수 있도록 도움
    - 허깅페이스의 transformers, accelerate 라이브러리와 연동해서 활용할 수 있음
- 단계 
    1. 사전 학습된 언어 모델 선택
    1. 언어 모델에 대한 점수 기록
    1. 불공정한 비교 보정 (점수를 보다 정확하게 평가)
- 종류
    - Prompt modifications ("Hard" prompt tuning, "Soft" prompt tuning, Prefix-tuning)
    - Adapter methods (LLaMA-Adapter)
    - Reparameterization (LoRA / Low rank adaption)


## Evaluation

### GLUE Benchmark
- 자연어 처리(NLP) 분야의 다양한 태스크들에 대한 표준화된 평가를 제공하기 위해 개발된 데이터셋과 벤치마크
    - 대표적인 NLP 태스크들을 모아서 구성되어 있으며, 다양한 언어 이해 작업에 대한 성능을 비교하고 평가
    - 총 9개의 Task로 되어 있음. train/val/test set이 주어짐
- NLP 태스크
    - MNLI(MultiNLI): 문장 페어 분류 태스크로, 주어진 두 문장 간의 관계를 예측하는 작업
    - QQP(Quora Question Pairs): 문장 페어 분류 태스크로, Quora의 질문 쌍에서 같은 의미의 질문 쌍을 예측하는 작업
    - SST-2(Sentiment Analysis): 문장 분류 태스크로, 영화 리뷰에서 감정(긍정/부정)을 예측하는 작업
    - CoLA(Corpus of Linguistic Acceptability): 문장 분류 태스크로, 주어진 문장이 문법적으로 올바른지를 예측하는 작업
    - MRPC(Microsoft Research Paraphrase Corpus): 문장 페어 분류 태스크로, 주어진 두 문장 간의 유사성을 예측하는 작업
    - RTE(Recognizing Textual Entailment): 문장 페어 분류 태스크로, 주어진 두 문장 간의 함의(Entailment) 여부를 예측하는 작업
    - WNLI(WIkiText-103-NLI): 문장 페어 분류 태스크로, 주어진 두 문장 간의 관계를 예측하는 작업


## LLM Fine-tuning 기법
https://velog.io/@nellcome/Instruction-Tuning%EC%9D%B4%EB%9E%80


### Prompt
- 주어진 작업을 인공지능 모델에게 지시하거나 원하는 결과를 얻기 위해 사용되는 문장 또는 텍스트 조각
    - 모델이 원하는 방식으로 작업을 수행하도록 지시하고, 사용자와의 상호작용을 시작하는데에 중요한 역할
    - 프롬프트의 선택과 설계는 모델의 동작과 결과에 큰 영향
- 구성
    - 지시어(Instruction)
        - 프롬프트는 작업을 수행하기 위한 지시어를 포함
        - 예를 들어 "번역해주세요."나 "질문에 답해주세요."와 같이 모델에게 수행할 작업을 명시적으로 지시하는 내용이 포함
    - 입력(Starting Input)
        - 일부 경우, 프롬프트에는 시작 입력(Starting Input)이 포함
        - 시작 입력은 모델의 답변을 시작하는데 사용되는 기본적인 문장 또는 텍스트 조각
    - 문맥(Context)
        - 대화 시스템에서는 이전 대화 내용을 포함하여 프롬프트를 구성하는 경우가 많음
        - 이전 대화의 문맥이 현재 프롬프트에 포함


### In-Conext Learning
- 대화식 AI 모델에서 사용되는 학습 방법 중 하나로, 모델이 대화의 문맥(context)을 고려하여 지속적으로 학습하고 개선하는 방식
    - 사용자와의 상호작용을 통해 모델이 실시간으로 학습하도록 돕는 데에 중요한 역할
    - 대규모 언어 모델에서 효과적
- 실제 사용자와의 상호작용을 통해 모델이 지속적으로 학습하고 개선하는 기술
    - 기존의 기계 학습 모델은 미리 정해진 대용량의 정적인 데이터셋을 사용하여 학습하는 방식. 이러한 한계를 극복하기 위해 개발된 방법. 
- 장점
    - 실시간 학습: 모델이 실제 사용자와 대화하면서 학습하기 때문에, 최신 정보와 상황에 적절하게 대응
    - 개인화: 사용자와의 상호작용을 통해 모델이 사용자에게 맞춤화된 답변을 생성하고 개선
    - 데이터 편향 감소: In-Context Learning은 실제 사용자와의 상호작용을 통해 다양한 데이터를 수집하므로 데이터 편향을 감소


### Instruction Tuning
- 다중 작업(feintuning)을 포함하는 대화형 AI 모델을 개선하기 위해 사용되는 학습 방법
    - user의 의도는 다른 답변을 주는 문제점을 해결
    - 기존의 대화형 AI 모델은 다양한 작업에 대해 미리 학습된 모델을 사용하여 다양한 요청에 대응 > 사용자의 의도를 제대로 이해를 못 할 수도 있음 > `사용자의 지시사항을 포함한 추가적인 지시 사항을 모델에 전달하고, 모델이 이러한 지시를 따르도록 강조적으로 학습`
- 과정
    1. 데이터셋 준비
        - Instruction Tuning은 사용자의 요구사항에 대한 지시사항이 포함된 데이터셋을 사용하여 모델을 학습
        - 이 데이터셋은 더 정확하고 명확한 지시를 제공하며, 모델이 원하는 대답을 생성하도록 도움
    1. Multi-task Finetuning
        - Instruction Tuning은 다중 작업(fineturning)을 활용하여 모델을 학습
        - 여러 작업에 대해 동시에 학습하면서, 특히 지시사항을 따르도록 강조하여 모델을 개선
    1. 지시 사항 강조
        - 지시 사항이 포함된 대화 형식으로 모델을 학습시키면서, 지시 사항을 더 잘 이해하고 지켜지도록 학습하도록 유도



