머신러닝_엔지니어_실무

# 1. 머신러닝 파이프라인
- 파이프라인: 잘 정의된 프로세스 > 일부 프로세스를 자동화하여 시간과 비용을 절감
    - 생산성 향상
    - 예측 가능한 품질
    - 장애 대응능력 향상 (머신러닝의 경우, 장애라는 개념이 조금 특이함)
- 머신러닝 모델의 배포는 쉽지만, **ML 모델을 운영하는 것이 어려움 (비용이 큼)**

#### 전통적인 소프트웨어 엔지니어와 비교
- 미래의 나를 위해서 지금이 힘들더라도 사전에 준비하는 것
    - 예) 리팩토링, 종속성 제거, 단위 테스트, API 강화, 문서화 등
- ML 시스템에서는 추상화(Abstraction)의 경계가 무너짐
    - 기존에 알려진 코드 수준의 방식으로 문제 해결이 어려움
    - 하나의 거대한 모델이 있을 때, 비슷하게 모델을 정의할 수가 없음

### 머신러닝 문제의 특징
- 어려운 ML 문제
    - 데이터의 변화가 **빠르게** 일어남 (주단위)
    - 재학습 주기
        - **모델 성능 저하**
        - 더 많은 데이터로 모델 성능 개선
        - 시스템 변화
    - 라벨링
        - **직접적인 피드백**(추천 시스템의 경우, 유저의 피드백이 주기적으로 나타남)
        - 수집한 데이터
 - 머신러닝 프로그래밍 문제의 특징
    코딩 | 엔지니어링
    --|--
    고정된 데이터셋 | 진화하는 데이터셋과 metric
    통합 불가한 아티팩트 | 재사용 가능한 모델
    문제 정의 없음 | 문제 정의, 찾기 쉬운 아티팩트
    검증되지 않은 데이터셋/모델 | 예상치, 데이터 검증, 모델 검증
    편향된 데이터셋/아티팩트 | 설명 가능하고, 공정한 데이터셋/아티팩트
    ... | 시각화, 요약, 이해, 아티팩트 형상 관리
- ML은 본질적으로 실험임
    - 다른 feature, 알고리즘, 모델링 기술 및 파라미터 구성을 시도해서 가능한 빨리 문제점에 가장 적합한 것은 찾음
    - 무엇이 효과가 있는지 아닌지를 추적하고, **코드 재사용을 극대화/유지**
- ML 모델은 코딩 뿐만 아니라, 데이터 때문에 성능이 저하될 수 있음
    - 데이터 통계치를 추적하고, 모니터링해서 문제를 인지해야 함

### MLOps의 핵심 문제
- 배포 트리거
    - 요청 시: 파이프라인 수동 실행
    - 일정 기준: 새 데이터는 매일, 매주 또는 매월
    - 새 학습 데이터: 새 데이터가 들어오는 경우
    - 모델 성능 저하: 눈에 띄는 성능 저하가 있을 경우
    - **데이터 분포의 중요한 변화(Concept Drift)**: 피쳐의 데이터 분포에 큰 변화가 있으면 모델이 오래되었다는 것을 뜻함
- 진화하는 데이터셋/Metric ... 이것에 부합하는 ML 모델을 새롭게 배포해야 함
- CI/CD/**CT(Continuous Training!)**

## MLOps 성숙도 레벨

#### Level 0
- 수동, 대화식 프로세스
- ML과 운영의 분리
- 드문 릴리즈 반복, CI/CD 없음
- 성능 모니터링 부족

<img src="https://user-images.githubusercontent.com/40620421/212064605-d64eafca-5efd-4f56-b2cf-c0fd7b62e33d.png" width=700>

#### Level 1
- 빠른 실험
- 프로덕션 모델의 CT
- 실험 운영 환경의 조화

<img src="https://user-images.githubusercontent.com/40620421/212064630-089afbcc-53c9-4444-82ba-2c335099c3ea.png" width=700>

1. Feature Store에서 데이터 추출 후, 모델 실험 (Orchestrated experiment)
1. 실험된 모델을 Pipeline으로 패키징
1. Pipeline이 배포되어 트리거가 발생하면 Pipeline을 따라 학습 진행 후 배포
1. 성능 모니터링을 통해서 트리거 발생 또는 모델 실험 진행

#### Level 2
- 자동화된 CI/CD 시스템이 필요
- 이를 통해 데이터 과학자는 Feature Engineering, 모델 아키텍처 및 하이퍼 파라미터에 대한 새로운 아이디어를 신속하게 탐색 가능
<img src="https://user-images.githubusercontent.com/40620421/212064661-15618322-65b2-402d-9f4e-0e35f0ed7516.png" width=700>


## 머신러닝 파이프라인 단계
- 데이터 전처리 > 모델 학습 > 모델 분석 > 모델 배포를 포함
    - 수동으로 진행하다보면 오류가 발생하기 쉬움

<img src="https://user-images.githubusercontent.com/40620421/212074481-d228ab45-888b-475b-a820-be72c5b47e59.png" width=600>


### 데이터 수집/검증
- 모델 학습을 위해 원하는 데이터 형태에 맞는 데이터 수집 진행
    - Data Versioning을 통해서 모델이 어떤 데이터셋을 통해서 훈련했는지 로그를 잘 확인할 수 있음 (S3 등에서는 자동으로 진행)
- 데이터 검증 진행
    - 새모델 버전을 학습하기 전에 검증이 필요함
    - 데이터의 통계가 예상대로인지 확인하고, 이상치가 발생할 경우 데이터 과학자에게 경고하도록 설정
- data split를 진행 
    - 데이터 과학자마다 다른 split를 가질 수 있기 때문에 이때 진행함
    - 이때 라벨 값 등이 편향되지는 않았는지 확인
- 데이터 전처리 진행
    - 데이터를 학습에 활용하기 위해서 데이터를 미리 처리해야 함
    - 예시
        - 1-hot Encoding
        - 텍스트 문자를 인덱스로 변환하거나 토큰을 워드 벡터로 변환
        - 새로운 Feature 생성

### 모델 학습/분석/버전 관리
- 모델 학습 진행
    - 가장 낮은 오차를 사용하여 입력을 수행하고 출력을 예측하는 모델을 학습
    - 메모리는 한정되어 있기 때문에, 효율적인 학습을 시키는 것이 중요
- 모델 튜닝 진행
    - 파이프라인에 모델 튜닝을 포함하여 일부 튜닝하는 것이 좋음 (**Auto ML**)
- 모델 분석
    - 지표를 활용하여 모델의 예측이 공정한지 확인
- 모델 버전 관리
    - 어떤 데이터셋, 하이퍼 파라미터, 모델을 사용하였는지 관리
    - A/B Test를 위해서도 필요함

### 모델 배포/피드백
- 모델 배포 진행
    - 일회성 구현으로 구성된 모델이라면 모델 업데이트는 쉽지 않음
        - PyTorch는 형상 관리하기가 어려움
    - 모던한 모델 서버를 사용하면 서버 프로그램 코드를 작성하지 않고도 모델을 배포할 수 있음
        - REST/RPC 등의 여러 API 인터페이스를 제공하여 동일한 모델의 여러 버전을 동시에 호스트할 수도 있음 >> 이를 통해서 모델 A/B Test를 진행할 수 있음
        - 애플리케이션을 다시 배포하지 않고도 모델을 변경할 수 있음
- 피드백 루프 반복
    - 새로 배포된 모델의 효과와 성능을 측정할 수 있어야 함
    - 데이터 과학자가 새로운 모델 연구에 집중할 수 있도록 함


# ML 프로젝트 실험 관리

## 실험 관리
- 학습 때 마다 그 과정과 결과를 기록하는 것이 중요함
    - 저번에 학습이 잘 되었는데, lr이 몇이었지?
    - 저번에 썼던 논문, 재연이 안 되네?
    - 이 모델에 어떤 데이터셋을 사용했지?

### Weights and Biases
- ML 프로젝트 실험관리 솔루션
    - 웹 브라우저를 기반으로 구성되어 있음
    - 팀원과 함께 공유할 수 있음
    - 실험 결과 시각화
    - GPU 사용률 (어떤 모델이 GPU를 많이 점유하고 있는가?)

#### 실습
```py
import random

total_runs = 5
for run in range(total_runs):
    # 실험 시작
    wandb.init(
        project="basic-intro", 
        name=f"experiment_{run}", 
        config={
        "learning_rate": 0.02,
        "architecture": "CNN",
        "dataset": "CIFAR-100",
        "epochs": 10,
        })
  
    epochs = 10
    offset = random.random() / 5
    for epoch in range(2, epochs):
        acc = 1 - 2 ** -epoch - random.random() / epoch - offset
        loss = 2 ** -epoch + random.random() / epoch + offset
        
        # log를 남김
        wandb.log({"acc": acc, "loss": loss})
    
    # 실험의 결과를 웹 사이트로 링크로 알려준다
    wandb.finish()
```

## 하이퍼 파라미터 최적화
