<p align="center">
<img src="https://user-images.githubusercontent.com/40620421/183598835-ebf8d4ba-354d-4b02-8ccd-a7d1f2d29018.jpg" width="300">

본 내용은 해당 도서를 참고하여 정리한 내용입니다.
</p>

<br/>
<br/>
<br/>

# 빅데이터의 기초 지식

## 1. 빅데이터의 정착
'빅데이터'의 역사에 대해 알아보자

### 빅데이터 기술의 요구 (Hadoop, No SQL, DW)
- RDB로는 취급할 수 없을 만큼 대량의 데이터가 쌓임
    - RDB는 데이터가 증가함에 따라 소프트웨어, 하드웨어적 비용이 비싸진다.

#### Hadoop
- 다수의 컴퓨터에서 대량의 데이터 처리
- 초기 `MapReduce`를 참고하여 제작됨
- `Java`로 개발되어 SQL에 익숙한 분석가가 활용하기 어려웠음 → Hive 도입
    - Java의 GC가 가장 성능이 좋음 (빅데이터를 다루기에 메모리 관점 중요)
    - Java가 디버깅이 쉬움
    - Hadoop이 Nutch 프로그램에서 발전 되었는데, 그것이 Java 기반
- 데이터를 나중에 집계하는 것이 목표

#### No SQL
- RDB에 비해 고속의 읽기, 쓰기가 가능하고, 분산 처리에 뛰어남
- 앱에서 온라인으로 접속하는 DB → `NoSQL DB에 기록하고 Hadoop으로 분산처리하자`

#### Data Warehouse(DW)
- 이전부터 데이터 분석을 위해 도입됨
- 분산 시스템의 발전에 따라 Hadoop과 연결하여 사용
    - DW의 데이터 확장성이 좋지 않음
    - 데이터 처리는 Hadoop이 진행하고, 비교적 작고 중요한 데이터만 DW에 넣는 식

## 2. 데이터 분석 기반의 빅데이터
- 기존의 DW와는 다르게 `다수의 분산 시스템을 조합`하여 `확장성`이 뛰어난 데이터 처리 구조를 만든다.

<img src = "https://user-images.githubusercontent.com/40620421/183667473-3d197fb6-be60-4b96-9ca6-a3c1773ca79a.png" width="450">

### 데이터 파이프라인
- 차례대로 전달해나가는 데이터로 구성된 시스템
- 데이터를 수집하여 무엇을 하고 싶은지에 따라 변화됨

#### 데이터 수집
- 데이터 파이프라인의 시작
- 데이터는 여러 장소에서 발생하고 각각 다른 형태를 가짐 (데이터 처리 필요)
- 데이터 처리 방법
    - 스트림 처리
        - 차례차례로 생성되는 데이터를 끊임없이 보냄 (30분간의 데이터를 집계)
        - 장기적인 데이터 처리에는 적합하지 않음
    - 배치 처리
        - 대량의 데이터를 저장하고, 처리하는 데 적합함
        - 분산 시스템 활용 (4, 5)

#### 분산 스토리지
- 여러 컴퓨터와 디스크로부터 구성된 스토리지 시스템
- 확장성이 높은 제품을 활용해야함
- Amazon S3, NoSQL DB

#### 분산 데이터 처리
- 데이터 분석을 위해 데이터를 가공하여 외부 DB에 저장
- MapReduce가 대표적이며, 많은 컴퓨터 자원을 필요로 한다
- 빅데이터를 SQL로 집계하기 위한 방법
    1. 쿼리엔진 도입 (Hive)
    1. 외부 DW를 이용. ETL과정을 거침.

#### 워크플로 관리
- 데이터 파이프라인의 동작을 관리하기 위함
- 배치 일정 관리 및 오류 발생 보고를 목적으로 사용됨

### DW와 데이터 마트
- `대량의 데이터를 장기 보존`하는 것에 최적화
    - 소량의 데이터를 자주 쓰고 읽는 데는 적합하지 않음
    - 하루가 끝날 때 정리하여 쓰고, 야간 시간대에 집계하여 보고서 작성
- 데이터 소스의 형태가 모두 다르기 때문에 `ETL 프로세스`활용
- DW는 중요한 데이터 처리에 사용되기에 과부하를 조심해야함 → 세분화된 니즈에 따라 구성된 `데이터 마트`구축
- DW 중심의 파이프라인에서는 테이블 설계와 ETL 프로세스가 중요

<img src="https://upload.wikimedia.org/wikipedia/commons/d/d8/Data_Warehouse_Feeding_Data_Mart.jpg" width="500">

### Data Lake
- 모든 데이터를 원래의 형태로 축척하고 나중에 그것을 필요에 따라 가공하는 구조
    - 모든 데이터가 DW를 가정해서 만들어지지 않음
- 필요한 데이터는 `데이터 마트`에 정리
    - Data Lake는 단순 분산 스토리지이기 때문에 데이터를 가공할 수 없음
    - MapReduce 등의 분산 데이터 처리 기술이 필요


